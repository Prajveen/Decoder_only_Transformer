{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e257275-c60b-45f1-9da6-9150213db888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /Users/beesamprajveenkumar/anaconda3/lib/python3.11/site-packages (1.26.3)\n",
      "Requirement already satisfied: pandas in /Users/beesamprajveenkumar/anaconda3/lib/python3.11/site-packages (2.0.3)\n",
      "Requirement already satisfied: datasets in /Users/beesamprajveenkumar/anaconda3/lib/python3.11/site-packages (2.18.0)\n",
      "Requirement already satisfied: sentencepiece in /Users/beesamprajveenkumar/anaconda3/lib/python3.11/site-packages (0.2.0)\n",
      "Requirement already satisfied: torch in /Users/beesamprajveenkumar/anaconda3/lib/python3.11/site-packages (2.2.1)\n",
      "Requirement already satisfied: matplotlib in /Users/beesamprajveenkumar/anaconda3/lib/python3.11/site-packages (3.8.2)\n",
      "Requirement already satisfied: transformers in /Users/beesamprajveenkumar/anaconda3/lib/python3.11/site-packages (4.38.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/beesamprajveenkumar/anaconda3/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/beesamprajveenkumar/anaconda3/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/beesamprajveenkumar/anaconda3/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: filelock in /Users/beesamprajveenkumar/anaconda3/lib/python3.11/site-packages (from datasets) (3.13.3)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /Users/beesamprajveenkumar/anaconda3/lib/python3.11/site-packages (from datasets) (15.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in /Users/beesamprajveenkumar/anaconda3/lib/python3.11/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/beesamprajveenkumar/anaconda3/lib/python3.11/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/beesamprajveenkumar/anaconda3/lib/python3.11/site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /Users/beesamprajveenkumar/anaconda3/lib/python3.11/site-packages (from datasets) (4.65.0)\n",
      "Requirement already satisfied: xxhash in /Users/beesamprajveenkumar/anaconda3/lib/python3.11/site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: multiprocess in /Users/beesamprajveenkumar/anaconda3/lib/python3.11/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /Users/beesamprajveenkumar/anaconda3/lib/python3.11/site-packages (from datasets) (2024.2.0)\n",
      "Requirement already satisfied: aiohttp in /Users/beesamprajveenkumar/anaconda3/lib/python3.11/site-packages (from datasets) (3.8.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in /Users/beesamprajveenkumar/anaconda3/lib/python3.11/site-packages (from datasets) (0.21.4)\n",
      "Requirement already satisfied: packaging in /Users/beesamprajveenkumar/anaconda3/lib/python3.11/site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/beesamprajveenkumar/anaconda3/lib/python3.11/site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/beesamprajveenkumar/anaconda3/lib/python3.11/site-packages (from torch) (4.10.0)\n",
      "Requirement already satisfied: sympy in /Users/beesamprajveenkumar/anaconda3/lib/python3.11/site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in /Users/beesamprajveenkumar/anaconda3/lib/python3.11/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/beesamprajveenkumar/anaconda3/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/beesamprajveenkumar/anaconda3/lib/python3.11/site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/beesamprajveenkumar/anaconda3/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/beesamprajveenkumar/anaconda3/lib/python3.11/site-packages (from matplotlib) (4.47.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/beesamprajveenkumar/anaconda3/lib/python3.11/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: pillow>=8 in /Users/beesamprajveenkumar/anaconda3/lib/python3.11/site-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/beesamprajveenkumar/anaconda3/lib/python3.11/site-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/beesamprajveenkumar/anaconda3/lib/python3.11/site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Users/beesamprajveenkumar/anaconda3/lib/python3.11/site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/beesamprajveenkumar/anaconda3/lib/python3.11/site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/beesamprajveenkumar/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/beesamprajveenkumar/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/beesamprajveenkumar/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/beesamprajveenkumar/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/beesamprajveenkumar/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/beesamprajveenkumar/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/beesamprajveenkumar/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/beesamprajveenkumar/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/beesamprajveenkumar/anaconda3/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/beesamprajveenkumar/anaconda3/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/beesamprajveenkumar/anaconda3/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (2023.11.17)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/beesamprajveenkumar/anaconda3/lib/python3.11/site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/beesamprajveenkumar/anaconda3/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy pandas datasets sentencepiece torch matplotlib transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d97b45a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sentencepiece'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_from_disk\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msentencepiece\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mspm\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconcurrent\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfutures\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ThreadPoolExecutor\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sentencepiece'"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "import pandas as pd\n",
    "import sentencepiece as spm\n",
    "from PIL import Image\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "from io import BytesIO\n",
    "import io\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import math\n",
    "from torch import nn\n",
    "from datasets import load_dataset\n",
    "from transformers import GPT2Tokenizer, GPT2TokenizerFast\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3477e81c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"roneneldan/TinyStories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1341c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 2119719\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 21990\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4d5e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = dataset['train']\n",
    "validation_data = dataset['validation']\n",
    "\n",
    "train_df = train_data.to_pandas()\n",
    "validation_df = validation_data.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cae17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()\n",
    "train_df = train_df[:4000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7b546e",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad5038b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained GPT-2 tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55dbcdc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 50257\n"
     ]
    }
   ],
   "source": [
    "# Get the vocabulary size\n",
    "vocab_size = len(tokenizer)\n",
    "print(\"Vocabulary size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bd18f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1106 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "#tokenizing the ids of the captions and including it into df\n",
    "train_df['text_tokens_ids'] = train_df['text'].apply(lambda x: tokenizer.encode(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856dd4ea-1189-45a5-9617-a9b554ce2d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4000 entries, 0 to 3999\n",
      "Data columns (total 2 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   text             4000 non-null   object\n",
      " 1   text_tokens_ids  4000 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 62.6+ KB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ca6541",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_tokens_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One day, a little girl named Lily found a need...</td>\n",
       "      <td>[3198, 1110, 11, 257, 1310, 2576, 3706, 20037,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Once upon a time, there was a little car named...</td>\n",
       "      <td>[7454, 2402, 257, 640, 11, 612, 373, 257, 1310...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>One day, a little fish named Fin was swimming ...</td>\n",
       "      <td>[3198, 1110, 11, 257, 1310, 5916, 3706, 4463, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Once upon a time, in a land full of trees, the...</td>\n",
       "      <td>[7454, 2402, 257, 640, 11, 287, 257, 1956, 133...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Once upon a time, there was a little girl name...</td>\n",
       "      <td>[7454, 2402, 257, 640, 11, 612, 373, 257, 1310...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  One day, a little girl named Lily found a need...   \n",
       "1  Once upon a time, there was a little car named...   \n",
       "2  One day, a little fish named Fin was swimming ...   \n",
       "3  Once upon a time, in a land full of trees, the...   \n",
       "4  Once upon a time, there was a little girl name...   \n",
       "\n",
       "                                     text_tokens_ids  \n",
       "0  [3198, 1110, 11, 257, 1310, 2576, 3706, 20037,...  \n",
       "1  [7454, 2402, 257, 640, 11, 612, 373, 257, 1310...  \n",
       "2  [3198, 1110, 11, 257, 1310, 5916, 3706, 4463, ...  \n",
       "3  [7454, 2402, 257, 640, 11, 287, 257, 1956, 133...  \n",
       "4  [7454, 2402, 257, 640, 11, 612, 373, 257, 1310...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc83fe68",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc14044",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset class\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, df, max_seq_len=20):\n",
    "        self.df = df\n",
    "        self.max_seq_len = max_seq_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        token_ids = self.df['text_tokens_ids'][idx] \n",
    "\n",
    "        # #Preprocess the captions for gpt-2\n",
    "        SOS = tokenizer.bos_token_id\n",
    "        EOS = tokenizer.eos_token_id\n",
    "        \n",
    "        input_text = token_ids.copy()\n",
    "        input_text.insert(0, SOS)\n",
    "\n",
    "        target_text = token_ids.copy()\n",
    "        target_text.append(EOS)\n",
    "        \n",
    "        cap_len = len(input_text)\n",
    "        pad_len = self.max_seq_len - cap_len\n",
    "        mask = []\n",
    "\n",
    "\n",
    "        if pad_len > 0:\n",
    "            zero_pad = [0] * pad_len\n",
    "            input_text.extend(zero_pad)\n",
    "            input_text_padded = input_text\n",
    "            \n",
    "            target_text.extend(zero_pad)\n",
    "            target_text_padded = target_text\n",
    "\n",
    "            mask.extend([1] * cap_len)\n",
    "            mask.extend([0] * pad_len)\n",
    "        else:\n",
    "            input_text_padded = input_text[:self.max_seq_len]\n",
    "            target_text_padded = target_text[:self.max_seq_len]\n",
    "            mask.extend([1] * self.max_seq_len)\n",
    "\n",
    "        return {\n",
    "            'input_tokens' : torch.tensor(input_text_padded),\n",
    "            'target_tokens': torch.tensor(target_text_padded),\n",
    "            'mask'         : torch.tensor(mask)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93479562",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c87dfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    return torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "def scaled_dot_product(q, k, v, pad_mask=None, atn_mask=False):\n",
    "    d_k = q.size()[-1]\n",
    "    \n",
    "    # Move q, k, and v tensors to the same device\n",
    "    q, k, v = q.to(get_device()), k.to(get_device()), v.to(get_device())\n",
    "    \n",
    "    scaled = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(d_k)\n",
    "    if atn_mask:\n",
    "        dia_mask = torch.full(scaled.size(), float('-inf'), device=get_device())\n",
    "        dia_mask = torch.triu(dia_mask, diagonal=1)\n",
    "        scaled += dia_mask\n",
    "    attention = F.softmax(scaled, dim=-1)\n",
    "    if pad_mask is not None:\n",
    "        pad_mask = pad_mask.unsqueeze(1).unsqueeze(1) * pad_mask.unsqueeze(1).unsqueeze(3)\n",
    "        # Move pad_mask to the same device\n",
    "        pad_mask = pad_mask.to(get_device())\n",
    "        attention = attention.masked_fill(pad_mask==0, 0)\n",
    "    values = torch.matmul(attention, v)\n",
    "    return values, attention\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_sequence_length):\n",
    "        super().__init__()\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, device=torch.device('cpu')):  # Pass device as an argument\n",
    "        even_i = torch.arange(0, self.d_model, 2).float().to(device)  # Move tensor to device\n",
    "        denominator = torch.pow(10000, even_i / self.d_model)\n",
    "        position = torch.arange(self.max_sequence_length, device=device).reshape(self.max_sequence_length, 1)  # Move tensor to device\n",
    "        even_PE = torch.sin(position / denominator)\n",
    "        odd_PE = torch.cos(position / denominator)\n",
    "        stacked = torch.stack([even_PE, odd_PE], dim=2)\n",
    "        PE = torch.flatten(stacked, start_dim=1, end_dim=2)\n",
    "        return PE\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.qkv_layer = nn.Linear(d_model , 3 * d_model)\n",
    "        self.linear_layer = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, x, pad_mask=None, atn_mask=False):\n",
    "        batch_size, sequence_length, d_model = x.shape\n",
    "        qkv = self.qkv_layer(x)\n",
    "        qkv = qkv.reshape(batch_size, sequence_length, self.num_heads, 3 * self.head_dim)\n",
    "        qkv = qkv.permute(0, 2, 1, 3)\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "        values, attention = scaled_dot_product(q, k, v, pad_mask, atn_mask = atn_mask)\n",
    "        values = values.reshape(batch_size, sequence_length, self.num_heads * self.head_dim)\n",
    "        out = self.linear_layer(values)\n",
    "        return out\n",
    "\n",
    "\n",
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, parameters_shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.parameters_shape=parameters_shape\n",
    "        self.eps=eps\n",
    "        self.gamma = nn.Parameter(torch.ones(parameters_shape))\n",
    "        self.beta =  nn.Parameter(torch.zeros(parameters_shape))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        dims = [-(i + 1) for i in range(len(self.parameters_shape))]\n",
    "        mean = inputs.mean(dim=dims, keepdim=True)\n",
    "        var = ((inputs - mean) ** 2).mean(dim=dims, keepdim=True)\n",
    "        std = (var + self.eps).sqrt()\n",
    "        y = (inputs - mean) / std\n",
    "        out = self.gamma * y + self.beta\n",
    "        return out\n",
    "\n",
    "  \n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, hidden, drop_prob=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, hidden)\n",
    "        self.linear2 = nn.Linear(hidden, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attention1 = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(p=drop_prob)\n",
    "        self.self_attention2 = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout2 = nn.Dropout(p=drop_prob)\n",
    "        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout3 = nn.Dropout(p=drop_prob)\n",
    "    \n",
    "    # Override the forward method to handle None values for pad_mask\n",
    "    def forward(self, y, pad_mask, atn_mask):\n",
    "        _y = y\n",
    "        \n",
    "        # Check if pad_mask is None before attempting to move it to device\n",
    "        if pad_mask is not None:\n",
    "            pad_mask = pad_mask.to(get_device())\n",
    "        \n",
    "        y = self.self_attention1(y, pad_mask, atn_mask)\n",
    "        y = self.dropout1(y) \n",
    "        y = self.norm1(y + _y) \n",
    "        _y = y\n",
    "        \n",
    "        y = self.ffn(y) \n",
    "        y = self.dropout3(y) \n",
    "        y = self.norm3(y + _y) \n",
    "        return y\n",
    "\n",
    "class SequentialDecoder(nn.Sequential):\n",
    "    def forward(self, *inputs):\n",
    "        y, pad_mask, atn_mask = inputs\n",
    "        for module in self._modules.values():\n",
    "            y = module(y, pad_mask, atn_mask)\n",
    "        return y\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 d_model, \n",
    "                 ffn_hidden, \n",
    "                 num_heads, \n",
    "                 drop_prob, \n",
    "                 num_layers):\n",
    "        super().__init__()\n",
    "        self.layers = SequentialDecoder(*[DecoderLayer(d_model, ffn_hidden, num_heads, drop_prob) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, y, pad_mask = None, atn_mask = True):\n",
    "        y = self.layers(y, pad_mask, atn_mask)\n",
    "        return y\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, \n",
    "                d_model, \n",
    "                ffn_hidden, \n",
    "                num_heads, \n",
    "                drop_prob, \n",
    "                num_layers,\n",
    "                vocab_size               \n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.dec_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.dec_pos_encoding = PositionalEncoding(d_model, 1)\n",
    "        \n",
    "        self.decoder = Decoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers)\n",
    "        self.linear = nn.Linear(d_model, vocab_size)\n",
    "        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "    def forward(self, input_tokens, pad_mask=None, atn_mask=True):\n",
    "        print(\"Input Tokens Shape:\", input_tokens.shape)\n",
    "\n",
    "        # Move input tensors to the appropriate device\n",
    "        input_tokens = input_tokens.to(self.device)\n",
    "        pad_mask = pad_mask.to(self.device) if pad_mask is not None else None\n",
    "        max_sequence_length = input_tokens.shape[1]  # Fix this line to get the correct sequence length\n",
    "        print(\"Max Sequence Length:\", max_sequence_length)\n",
    "\n",
    "        # Compute token embeddings\n",
    "        token_embeddings = self.dec_embedding(input_tokens) \n",
    "        print(\"Token Embeddings Shape:\", token_embeddings.shape)\n",
    "\n",
    "        self.dec_pos_encoding = PositionalEncoding(d_model, max_sequence_length)\n",
    "        token_pos_encodings = self.dec_pos_encoding(device=self.device)  # Pass device argument\n",
    "        print(\"Token Position Encodings Shape:\", token_pos_encodings.shape)\n",
    "\n",
    "        token_embeddings_with_pos = token_embeddings + token_pos_encodings.unsqueeze(0)\n",
    "        print(\"Token Embeddings with Position Shape:\", token_embeddings_with_pos.shape)\n",
    "\n",
    "        # Perform the rest of the forward pass\n",
    "        out = self.decoder(token_embeddings_with_pos, pad_mask, atn_mask)\n",
    "        print(\"Decoder Output Shape:\", out.shape)\n",
    "\n",
    "        out = self.linear(out)\n",
    "        print(\"Linear Layer Output Shape:\", out.shape)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b0d8a2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocab_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m ffn_hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m\n\u001b[1;32m      6\u001b[0m num_layers \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m----> 7\u001b[0m vocab_size \u001b[38;5;241m=\u001b[39m \u001b[43mvocab_size\u001b[49m\n\u001b[1;32m      8\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vocab_size' is not defined"
     ]
    }
   ],
   "source": [
    "d_model = 8\n",
    "num_heads = 8\n",
    "drop_prob = 0.1\n",
    "batch_size = 2\n",
    "ffn_hidden = 8\n",
    "num_layers = 2\n",
    "vocab_size = vocab_size\n",
    "num_epochs = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41ffa72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to get the device\n",
    "def get_device():\n",
    "    return torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257d13af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (dec_embedding): Embedding(50257, 8)\n",
       "  (dec_pos_encoding): PositionalEncoding()\n",
       "  (decoder): Decoder(\n",
       "    (layers): SequentialDecoder(\n",
       "      (0): DecoderLayer(\n",
       "        (self_attention1): MultiHeadAttention(\n",
       "          (qkv_layer): Linear(in_features=8, out_features=24, bias=True)\n",
       "          (linear_layer): Linear(in_features=8, out_features=8, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (self_attention2): MultiHeadAttention(\n",
       "          (qkv_layer): Linear(in_features=8, out_features=24, bias=True)\n",
       "          (linear_layer): Linear(in_features=8, out_features=8, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (ffn): PositionwiseFeedForward(\n",
       "          (linear1): Linear(in_features=8, out_features=8, bias=True)\n",
       "          (linear2): Linear(in_features=8, out_features=8, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (norm3): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): DecoderLayer(\n",
       "        (self_attention1): MultiHeadAttention(\n",
       "          (qkv_layer): Linear(in_features=8, out_features=24, bias=True)\n",
       "          (linear_layer): Linear(in_features=8, out_features=8, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (self_attention2): MultiHeadAttention(\n",
       "          (qkv_layer): Linear(in_features=8, out_features=24, bias=True)\n",
       "          (linear_layer): Linear(in_features=8, out_features=8, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (ffn): PositionwiseFeedForward(\n",
       "          (linear1): Linear(in_features=8, out_features=8, bias=True)\n",
       "          (linear2): Linear(in_features=8, out_features=8, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (norm3): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=8, out_features=50257, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Move the model to the appropriate device\n",
    "transformer = Transformer(d_model, ffn_hidden, num_heads, drop_prob, num_layers, vocab_size)\n",
    "transformer.to(get_device())  # Move the model to GPU if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475ef085",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MyDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mMyDataset\u001b[49m(train_df[:\u001b[38;5;241m4\u001b[39m], max_seq_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m)\n\u001b[1;32m      2\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MyDataset' is not defined"
     ]
    }
   ],
   "source": [
    "dataset = MyDataset(train_df[:4], max_seq_len=128)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538572da-14a2-477e-a906-548be94bd682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 855,905 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(transformer):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50656624",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [1/100], Batch [1/2], Loss: 11.043807029724121\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [1/100], Total Loss: 11.014247417449951\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [2/100], Batch [1/2], Loss: 11.020112037658691\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [2/100], Total Loss: 10.989092350006104\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [3/100], Batch [1/2], Loss: 11.012547492980957\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [3/100], Total Loss: 10.971372127532959\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [4/100], Batch [1/2], Loss: 10.968012809753418\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [4/100], Total Loss: 10.939600467681885\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [5/100], Batch [1/2], Loss: 10.94296646118164\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [5/100], Total Loss: 10.91631269454956\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [6/100], Batch [1/2], Loss: 10.935931205749512\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [6/100], Total Loss: 10.90320348739624\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [7/100], Batch [1/2], Loss: 10.911112785339355\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [7/100], Total Loss: 10.879792213439941\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [8/100], Batch [1/2], Loss: 10.88294792175293\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [8/100], Total Loss: 10.85165023803711\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [9/100], Batch [1/2], Loss: 10.863028526306152\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [9/100], Total Loss: 10.833808898925781\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [10/100], Batch [1/2], Loss: 10.840953826904297\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [10/100], Total Loss: 10.814179420471191\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [11/100], Batch [1/2], Loss: 10.823180198669434\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [11/100], Total Loss: 10.79518985748291\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [12/100], Batch [1/2], Loss: 10.803545951843262\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [12/100], Total Loss: 10.775875568389893\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [13/100], Batch [1/2], Loss: 10.771695137023926\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [13/100], Total Loss: 10.741174697875977\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [14/100], Batch [1/2], Loss: 10.753205299377441\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [14/100], Total Loss: 10.729072570800781\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [15/100], Batch [1/2], Loss: 10.742462158203125\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [15/100], Total Loss: 10.708746910095215\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [16/100], Batch [1/2], Loss: 10.706104278564453\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [16/100], Total Loss: 10.67526912689209\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [17/100], Batch [1/2], Loss: 10.67979907989502\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [17/100], Total Loss: 10.654748916625977\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [18/100], Batch [1/2], Loss: 10.661369323730469\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [18/100], Total Loss: 10.63608980178833\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [19/100], Batch [1/2], Loss: 10.635469436645508\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [19/100], Total Loss: 10.603147506713867\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [20/100], Batch [1/2], Loss: 10.618721961975098\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [20/100], Total Loss: 10.591056823730469\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [21/100], Batch [1/2], Loss: 10.585227966308594\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [21/100], Total Loss: 10.557880878448486\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [22/100], Batch [1/2], Loss: 10.566267967224121\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [22/100], Total Loss: 10.533430576324463\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [23/100], Batch [1/2], Loss: 10.539310455322266\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [23/100], Total Loss: 10.504138946533203\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [24/100], Batch [1/2], Loss: 10.51077651977539\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [24/100], Total Loss: 10.48091459274292\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [25/100], Batch [1/2], Loss: 10.493712425231934\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [25/100], Total Loss: 10.45842695236206\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [26/100], Batch [1/2], Loss: 10.463346481323242\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [26/100], Total Loss: 10.428221702575684\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [27/100], Batch [1/2], Loss: 10.41747760772705\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [27/100], Total Loss: 10.391647815704346\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [28/100], Batch [1/2], Loss: 10.392203330993652\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [28/100], Total Loss: 10.368504047393799\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [29/100], Batch [1/2], Loss: 10.379212379455566\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [29/100], Total Loss: 10.345560073852539\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [30/100], Batch [1/2], Loss: 10.333179473876953\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [30/100], Total Loss: 10.299805164337158\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [31/100], Batch [1/2], Loss: 10.305558204650879\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [31/100], Total Loss: 10.275075435638428\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [32/100], Batch [1/2], Loss: 10.276749610900879\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [32/100], Total Loss: 10.250935554504395\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [33/100], Batch [1/2], Loss: 10.230934143066406\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [33/100], Total Loss: 10.200455665588379\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [34/100], Batch [1/2], Loss: 10.203752517700195\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [34/100], Total Loss: 10.173939228057861\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [35/100], Batch [1/2], Loss: 10.157361030578613\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [35/100], Total Loss: 10.13838005065918\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [36/100], Batch [1/2], Loss: 10.124725341796875\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [36/100], Total Loss: 10.095908164978027\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [37/100], Batch [1/2], Loss: 10.064166069030762\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [37/100], Total Loss: 10.049413204193115\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [38/100], Batch [1/2], Loss: 10.052781105041504\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [38/100], Total Loss: 10.026072025299072\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [39/100], Batch [1/2], Loss: 9.997507095336914\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [39/100], Total Loss: 9.982219219207764\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [40/100], Batch [1/2], Loss: 9.956622123718262\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [40/100], Total Loss: 9.935168266296387\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [41/100], Batch [1/2], Loss: 9.925333023071289\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [41/100], Total Loss: 9.897393226623535\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [42/100], Batch [1/2], Loss: 9.883585929870605\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [42/100], Total Loss: 9.860559463500977\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [43/100], Batch [1/2], Loss: 9.838807106018066\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [43/100], Total Loss: 9.821328163146973\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [44/100], Batch [1/2], Loss: 9.800521850585938\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [44/100], Total Loss: 9.770357608795166\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [45/100], Batch [1/2], Loss: 9.748751640319824\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [45/100], Total Loss: 9.73253059387207\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [46/100], Batch [1/2], Loss: 9.724383354187012\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [46/100], Total Loss: 9.701138973236084\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [47/100], Batch [1/2], Loss: 9.675217628479004\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [47/100], Total Loss: 9.65065050125122\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [48/100], Batch [1/2], Loss: 9.629242897033691\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [48/100], Total Loss: 9.606244564056396\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [49/100], Batch [1/2], Loss: 9.597314834594727\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [49/100], Total Loss: 9.567193031311035\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [50/100], Batch [1/2], Loss: 9.53493595123291\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [50/100], Total Loss: 9.511900424957275\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [51/100], Batch [1/2], Loss: 9.496402740478516\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [51/100], Total Loss: 9.471345901489258\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [52/100], Batch [1/2], Loss: 9.443861961364746\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [52/100], Total Loss: 9.423175811767578\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [53/100], Batch [1/2], Loss: 9.408369064331055\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [53/100], Total Loss: 9.38320779800415\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [54/100], Batch [1/2], Loss: 9.356856346130371\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [54/100], Total Loss: 9.331275939941406\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [55/100], Batch [1/2], Loss: 9.316859245300293\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [55/100], Total Loss: 9.293678760528564\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [56/100], Batch [1/2], Loss: 9.265687942504883\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [56/100], Total Loss: 9.248185634613037\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [57/100], Batch [1/2], Loss: 9.214635848999023\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [57/100], Total Loss: 9.201167583465576\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [58/100], Batch [1/2], Loss: 9.177836418151855\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [58/100], Total Loss: 9.158774852752686\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [59/100], Batch [1/2], Loss: 9.141172409057617\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [59/100], Total Loss: 9.110410690307617\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [60/100], Batch [1/2], Loss: 9.084939956665039\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [60/100], Total Loss: 9.056341648101807\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [61/100], Batch [1/2], Loss: 9.027462005615234\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [61/100], Total Loss: 9.005229949951172\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [62/100], Batch [1/2], Loss: 8.974003791809082\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [62/100], Total Loss: 8.96274709701538\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [63/100], Batch [1/2], Loss: 8.940727233886719\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [63/100], Total Loss: 8.913931369781494\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [64/100], Batch [1/2], Loss: 8.891982078552246\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [64/100], Total Loss: 8.877560138702393\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [65/100], Batch [1/2], Loss: 8.830732345581055\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [65/100], Total Loss: 8.810564994812012\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [66/100], Batch [1/2], Loss: 8.78543472290039\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [66/100], Total Loss: 8.767903327941895\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [67/100], Batch [1/2], Loss: 8.752118110656738\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [67/100], Total Loss: 8.719178199768066\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [68/100], Batch [1/2], Loss: 8.71567153930664\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [68/100], Total Loss: 8.683310508728027\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [69/100], Batch [1/2], Loss: 8.644917488098145\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [69/100], Total Loss: 8.618936538696289\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [70/100], Batch [1/2], Loss: 8.599660873413086\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [70/100], Total Loss: 8.57812213897705\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [71/100], Batch [1/2], Loss: 8.551403045654297\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [71/100], Total Loss: 8.526447296142578\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [72/100], Batch [1/2], Loss: 8.504753112792969\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [72/100], Total Loss: 8.485080242156982\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [73/100], Batch [1/2], Loss: 8.454120635986328\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [73/100], Total Loss: 8.433216094970703\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [74/100], Batch [1/2], Loss: 8.395791053771973\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [74/100], Total Loss: 8.378949165344238\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [75/100], Batch [1/2], Loss: 8.353113174438477\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [75/100], Total Loss: 8.323205471038818\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [76/100], Batch [1/2], Loss: 8.307991027832031\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [76/100], Total Loss: 8.278278350830078\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [77/100], Batch [1/2], Loss: 8.249099731445312\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [77/100], Total Loss: 8.227649211883545\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [78/100], Batch [1/2], Loss: 8.223986625671387\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [78/100], Total Loss: 8.188076496124268\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [79/100], Batch [1/2], Loss: 8.161383628845215\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [79/100], Total Loss: 8.128367900848389\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [80/100], Batch [1/2], Loss: 8.107497215270996\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [80/100], Total Loss: 8.083865642547607\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [81/100], Batch [1/2], Loss: 8.052202224731445\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [81/100], Total Loss: 8.02881145477295\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [82/100], Batch [1/2], Loss: 8.005605697631836\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [82/100], Total Loss: 7.977691888809204\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [83/100], Batch [1/2], Loss: 7.963094711303711\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [83/100], Total Loss: 7.934215784072876\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [84/100], Batch [1/2], Loss: 7.897987365722656\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [84/100], Total Loss: 7.874452352523804\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [85/100], Batch [1/2], Loss: 7.8639702796936035\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [85/100], Total Loss: 7.828958988189697\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [86/100], Batch [1/2], Loss: 7.803016185760498\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [86/100], Total Loss: 7.777226209640503\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [87/100], Batch [1/2], Loss: 7.736423492431641\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [87/100], Total Loss: 7.718918085098267\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [88/100], Batch [1/2], Loss: 7.7057623863220215\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [88/100], Total Loss: 7.6817851066589355\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [89/100], Batch [1/2], Loss: 7.65667724609375\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [89/100], Total Loss: 7.630331754684448\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [90/100], Batch [1/2], Loss: 7.596089839935303\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [90/100], Total Loss: 7.570485591888428\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [91/100], Batch [1/2], Loss: 7.559915065765381\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [91/100], Total Loss: 7.520030736923218\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [92/100], Batch [1/2], Loss: 7.502399444580078\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [92/100], Total Loss: 7.476082801818848\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [93/100], Batch [1/2], Loss: 7.45166015625\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [93/100], Total Loss: 7.420300483703613\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [94/100], Batch [1/2], Loss: 7.399921894073486\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [94/100], Total Loss: 7.368190288543701\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [95/100], Batch [1/2], Loss: 7.353481292724609\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [95/100], Total Loss: 7.320629835128784\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [96/100], Batch [1/2], Loss: 7.310793876647949\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [96/100], Total Loss: 7.284661531448364\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [97/100], Batch [1/2], Loss: 7.25267219543457\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [97/100], Total Loss: 7.223455190658569\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [98/100], Batch [1/2], Loss: 7.2228851318359375\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [98/100], Total Loss: 7.187126398086548\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [99/100], Batch [1/2], Loss: 7.1526007652282715\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [99/100], Total Loss: 7.129755973815918\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [100/100], Batch [1/2], Loss: 7.103110313415527\n",
      "Input Tokens Shape: torch.Size([2, 128])\n",
      "Max Sequence Length: 128\n",
      "Token Embeddings Shape: torch.Size([2, 128, 8])\n",
      "Token Position Encodings Shape: torch.Size([128, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([2, 128, 8])\n",
      "Decoder Output Shape: torch.Size([2, 128, 8])\n",
      "Linear Layer Output Shape: torch.Size([2, 128, 50257])\n",
      "Epoch [100/100], Total Loss: 7.08570671081543\n"
     ]
    }
   ],
   "source": [
    "# Define the optimizer and scheduler\n",
    "optim = torch.optim.Adam(transformer.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9)\n",
    "#scheduler = StepLR(optim, step_size=20, gamma=0.1)  # Learning rate decreases by a factor of 0.1 every 5 epochs\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Iterate over epochs\n",
    "for epoch in range(num_epochs):\n",
    "    transformer.train()  # Set the model to training mode\n",
    "    total_loss = 0\n",
    "    for batch_num, batch in enumerate(dataloader):\n",
    "        input_tokens, target_tokens, pad_mask = batch['input_tokens'], batch['target_tokens'], batch['mask']\n",
    "        \n",
    "        # Move tensors to GPU if available\n",
    "        input_tokens = input_tokens.to(get_device())\n",
    "        target_tokens = target_tokens.to(get_device())\n",
    "        pad_mask = pad_mask.to(get_device())\n",
    "        \n",
    "        optim.zero_grad()\n",
    "        predictions = transformer(input_tokens, pad_mask)\n",
    "        loss = criterion(predictions.view(-1, vocab_size), target_tokens.view(-1))\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Print loss after each batch\n",
    "        print_every = 50  # Define how often to print the loss\n",
    "        if batch_num % print_every == 0:\n",
    "           print(f'Epoch [{epoch + 1}/{num_epochs}], Batch [{batch_num + 1}/{len(dataloader)}], Loss: {loss.item()}')\n",
    "            \n",
    "    \n",
    "    # Print total loss after each epoch\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Total Loss: {total_loss/len(dataloader)}')\n",
    "    \n",
    "    # Update the learning rate\n",
    "    #scheduler.step()\n",
    "    \n",
    "    # Save the model after each epoch\n",
    "    model_path = '/workspace/transformer.pth'\n",
    "    torch.save(transformer.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289778ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Tokens Shape: torch.Size([1, 8])\n",
      "Max Sequence Length: 8\n",
      "Token Embeddings Shape: torch.Size([1, 8, 8])\n",
      "Token Position Encodings Shape: torch.Size([8, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([1, 8, 8])\n",
      "Decoder Output Shape: torch.Size([1, 8, 8])\n",
      "Linear Layer Output Shape: torch.Size([1, 8, 50257])\n",
      "Input Tokens Shape: torch.Size([1, 9])\n",
      "Max Sequence Length: 9\n",
      "Token Embeddings Shape: torch.Size([1, 9, 8])\n",
      "Token Position Encodings Shape: torch.Size([9, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([1, 9, 8])\n",
      "Decoder Output Shape: torch.Size([1, 9, 8])\n",
      "Linear Layer Output Shape: torch.Size([1, 9, 50257])\n",
      "Input Tokens Shape: torch.Size([1, 10])\n",
      "Max Sequence Length: 10\n",
      "Token Embeddings Shape: torch.Size([1, 10, 8])\n",
      "Token Position Encodings Shape: torch.Size([10, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([1, 10, 8])\n",
      "Decoder Output Shape: torch.Size([1, 10, 8])\n",
      "Linear Layer Output Shape: torch.Size([1, 10, 50257])\n",
      "Input Tokens Shape: torch.Size([1, 11])\n",
      "Max Sequence Length: 11\n",
      "Token Embeddings Shape: torch.Size([1, 11, 8])\n",
      "Token Position Encodings Shape: torch.Size([11, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([1, 11, 8])\n",
      "Decoder Output Shape: torch.Size([1, 11, 8])\n",
      "Linear Layer Output Shape: torch.Size([1, 11, 50257])\n",
      "Input Tokens Shape: torch.Size([1, 12])\n",
      "Max Sequence Length: 12\n",
      "Token Embeddings Shape: torch.Size([1, 12, 8])\n",
      "Token Position Encodings Shape: torch.Size([12, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([1, 12, 8])\n",
      "Decoder Output Shape: torch.Size([1, 12, 8])\n",
      "Linear Layer Output Shape: torch.Size([1, 12, 50257])\n",
      "Input Tokens Shape: torch.Size([1, 13])\n",
      "Max Sequence Length: 13\n",
      "Token Embeddings Shape: torch.Size([1, 13, 8])\n",
      "Token Position Encodings Shape: torch.Size([13, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([1, 13, 8])\n",
      "Decoder Output Shape: torch.Size([1, 13, 8])\n",
      "Linear Layer Output Shape: torch.Size([1, 13, 50257])\n",
      "Input Tokens Shape: torch.Size([1, 14])\n",
      "Max Sequence Length: 14\n",
      "Token Embeddings Shape: torch.Size([1, 14, 8])\n",
      "Token Position Encodings Shape: torch.Size([14, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([1, 14, 8])\n",
      "Decoder Output Shape: torch.Size([1, 14, 8])\n",
      "Linear Layer Output Shape: torch.Size([1, 14, 50257])\n",
      "Input Tokens Shape: torch.Size([1, 15])\n",
      "Max Sequence Length: 15\n",
      "Token Embeddings Shape: torch.Size([1, 15, 8])\n",
      "Token Position Encodings Shape: torch.Size([15, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([1, 15, 8])\n",
      "Decoder Output Shape: torch.Size([1, 15, 8])\n",
      "Linear Layer Output Shape: torch.Size([1, 15, 50257])\n",
      "Input Tokens Shape: torch.Size([1, 16])\n",
      "Max Sequence Length: 16\n",
      "Token Embeddings Shape: torch.Size([1, 16, 8])\n",
      "Token Position Encodings Shape: torch.Size([16, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([1, 16, 8])\n",
      "Decoder Output Shape: torch.Size([1, 16, 8])\n",
      "Linear Layer Output Shape: torch.Size([1, 16, 50257])\n",
      "Input Tokens Shape: torch.Size([1, 17])\n",
      "Max Sequence Length: 17\n",
      "Token Embeddings Shape: torch.Size([1, 17, 8])\n",
      "Token Position Encodings Shape: torch.Size([17, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([1, 17, 8])\n",
      "Decoder Output Shape: torch.Size([1, 17, 8])\n",
      "Linear Layer Output Shape: torch.Size([1, 17, 50257])\n",
      "Input Tokens Shape: torch.Size([1, 18])\n",
      "Max Sequence Length: 18\n",
      "Token Embeddings Shape: torch.Size([1, 18, 8])\n",
      "Token Position Encodings Shape: torch.Size([18, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([1, 18, 8])\n",
      "Decoder Output Shape: torch.Size([1, 18, 8])\n",
      "Linear Layer Output Shape: torch.Size([1, 18, 50257])\n",
      "Input Tokens Shape: torch.Size([1, 19])\n",
      "Max Sequence Length: 19\n",
      "Token Embeddings Shape: torch.Size([1, 19, 8])\n",
      "Token Position Encodings Shape: torch.Size([19, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([1, 19, 8])\n",
      "Decoder Output Shape: torch.Size([1, 19, 8])\n",
      "Linear Layer Output Shape: torch.Size([1, 19, 50257])\n",
      "Input Tokens Shape: torch.Size([1, 20])\n",
      "Max Sequence Length: 20\n",
      "Token Embeddings Shape: torch.Size([1, 20, 8])\n",
      "Token Position Encodings Shape: torch.Size([20, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([1, 20, 8])\n",
      "Decoder Output Shape: torch.Size([1, 20, 8])\n",
      "Linear Layer Output Shape: torch.Size([1, 20, 50257])\n",
      "Input Tokens Shape: torch.Size([1, 21])\n",
      "Max Sequence Length: 21\n",
      "Token Embeddings Shape: torch.Size([1, 21, 8])\n",
      "Token Position Encodings Shape: torch.Size([21, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([1, 21, 8])\n",
      "Decoder Output Shape: torch.Size([1, 21, 8])\n",
      "Linear Layer Output Shape: torch.Size([1, 21, 50257])\n",
      "Input Tokens Shape: torch.Size([1, 22])\n",
      "Max Sequence Length: 22\n",
      "Token Embeddings Shape: torch.Size([1, 22, 8])\n",
      "Token Position Encodings Shape: torch.Size([22, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([1, 22, 8])\n",
      "Decoder Output Shape: torch.Size([1, 22, 8])\n",
      "Linear Layer Output Shape: torch.Size([1, 22, 50257])\n",
      "Input Tokens Shape: torch.Size([1, 23])\n",
      "Max Sequence Length: 23\n",
      "Token Embeddings Shape: torch.Size([1, 23, 8])\n",
      "Token Position Encodings Shape: torch.Size([23, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([1, 23, 8])\n",
      "Decoder Output Shape: torch.Size([1, 23, 8])\n",
      "Linear Layer Output Shape: torch.Size([1, 23, 50257])\n",
      "Input Tokens Shape: torch.Size([1, 24])\n",
      "Max Sequence Length: 24\n",
      "Token Embeddings Shape: torch.Size([1, 24, 8])\n",
      "Token Position Encodings Shape: torch.Size([24, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([1, 24, 8])\n",
      "Decoder Output Shape: torch.Size([1, 24, 8])\n",
      "Linear Layer Output Shape: torch.Size([1, 24, 50257])\n",
      "Input Tokens Shape: torch.Size([1, 25])\n",
      "Max Sequence Length: 25\n",
      "Token Embeddings Shape: torch.Size([1, 25, 8])\n",
      "Token Position Encodings Shape: torch.Size([25, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([1, 25, 8])\n",
      "Decoder Output Shape: torch.Size([1, 25, 8])\n",
      "Linear Layer Output Shape: torch.Size([1, 25, 50257])\n",
      "Input Tokens Shape: torch.Size([1, 26])\n",
      "Max Sequence Length: 26\n",
      "Token Embeddings Shape: torch.Size([1, 26, 8])\n",
      "Token Position Encodings Shape: torch.Size([26, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([1, 26, 8])\n",
      "Decoder Output Shape: torch.Size([1, 26, 8])\n",
      "Linear Layer Output Shape: torch.Size([1, 26, 50257])\n",
      "Input Tokens Shape: torch.Size([1, 27])\n",
      "Max Sequence Length: 27\n",
      "Token Embeddings Shape: torch.Size([1, 27, 8])\n",
      "Token Position Encodings Shape: torch.Size([27, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([1, 27, 8])\n",
      "Decoder Output Shape: torch.Size([1, 27, 8])\n",
      "Linear Layer Output Shape: torch.Size([1, 27, 50257])\n",
      "Input Tokens Shape: torch.Size([1, 28])\n",
      "Max Sequence Length: 28\n",
      "Token Embeddings Shape: torch.Size([1, 28, 8])\n",
      "Token Position Encodings Shape: torch.Size([28, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([1, 28, 8])\n",
      "Decoder Output Shape: torch.Size([1, 28, 8])\n",
      "Linear Layer Output Shape: torch.Size([1, 28, 50257])\n",
      "Input Tokens Shape: torch.Size([1, 29])\n",
      "Max Sequence Length: 29\n",
      "Token Embeddings Shape: torch.Size([1, 29, 8])\n",
      "Token Position Encodings Shape: torch.Size([29, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([1, 29, 8])\n",
      "Decoder Output Shape: torch.Size([1, 29, 8])\n",
      "Linear Layer Output Shape: torch.Size([1, 29, 50257])\n",
      "Input Tokens Shape: torch.Size([1, 30])\n",
      "Max Sequence Length: 30\n",
      "Token Embeddings Shape: torch.Size([1, 30, 8])\n",
      "Token Position Encodings Shape: torch.Size([30, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([1, 30, 8])\n",
      "Decoder Output Shape: torch.Size([1, 30, 8])\n",
      "Linear Layer Output Shape: torch.Size([1, 30, 50257])\n",
      "Input Tokens Shape: torch.Size([1, 31])\n",
      "Max Sequence Length: 31\n",
      "Token Embeddings Shape: torch.Size([1, 31, 8])\n",
      "Token Position Encodings Shape: torch.Size([31, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([1, 31, 8])\n",
      "Decoder Output Shape: torch.Size([1, 31, 8])\n",
      "Linear Layer Output Shape: torch.Size([1, 31, 50257])\n",
      "Input Tokens Shape: torch.Size([1, 32])\n",
      "Max Sequence Length: 32\n",
      "Token Embeddings Shape: torch.Size([1, 32, 8])\n",
      "Token Position Encodings Shape: torch.Size([32, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([1, 32, 8])\n",
      "Decoder Output Shape: torch.Size([1, 32, 8])\n",
      "Linear Layer Output Shape: torch.Size([1, 32, 50257])\n",
      "Input Tokens Shape: torch.Size([1, 33])\n",
      "Max Sequence Length: 33\n",
      "Token Embeddings Shape: torch.Size([1, 33, 8])\n",
      "Token Position Encodings Shape: torch.Size([33, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([1, 33, 8])\n",
      "Decoder Output Shape: torch.Size([1, 33, 8])\n",
      "Linear Layer Output Shape: torch.Size([1, 33, 50257])\n",
      "Input Tokens Shape: torch.Size([1, 34])\n",
      "Max Sequence Length: 34\n",
      "Token Embeddings Shape: torch.Size([1, 34, 8])\n",
      "Token Position Encodings Shape: torch.Size([34, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([1, 34, 8])\n",
      "Decoder Output Shape: torch.Size([1, 34, 8])\n",
      "Linear Layer Output Shape: torch.Size([1, 34, 50257])\n",
      "Input Tokens Shape: torch.Size([1, 35])\n",
      "Max Sequence Length: 35\n",
      "Token Embeddings Shape: torch.Size([1, 35, 8])\n",
      "Token Position Encodings Shape: torch.Size([35, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([1, 35, 8])\n",
      "Decoder Output Shape: torch.Size([1, 35, 8])\n",
      "Linear Layer Output Shape: torch.Size([1, 35, 50257])\n",
      "Input Tokens Shape: torch.Size([1, 36])\n",
      "Max Sequence Length: 36\n",
      "Token Embeddings Shape: torch.Size([1, 36, 8])\n",
      "Token Position Encodings Shape: torch.Size([36, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([1, 36, 8])\n",
      "Decoder Output Shape: torch.Size([1, 36, 8])\n",
      "Linear Layer Output Shape: torch.Size([1, 36, 50257])\n",
      "Input Tokens Shape: torch.Size([1, 37])\n",
      "Max Sequence Length: 37\n",
      "Token Embeddings Shape: torch.Size([1, 37, 8])\n",
      "Token Position Encodings Shape: torch.Size([37, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([1, 37, 8])\n",
      "Decoder Output Shape: torch.Size([1, 37, 8])\n",
      "Linear Layer Output Shape: torch.Size([1, 37, 50257])\n",
      "Input Tokens Shape: torch.Size([1, 38])\n",
      "Max Sequence Length: 38\n",
      "Token Embeddings Shape: torch.Size([1, 38, 8])\n",
      "Token Position Encodings Shape: torch.Size([38, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([1, 38, 8])\n",
      "Decoder Output Shape: torch.Size([1, 38, 8])\n",
      "Linear Layer Output Shape: torch.Size([1, 38, 50257])\n",
      "Input Tokens Shape: torch.Size([1, 39])\n",
      "Max Sequence Length: 39\n",
      "Token Embeddings Shape: torch.Size([1, 39, 8])\n",
      "Token Position Encodings Shape: torch.Size([39, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([1, 39, 8])\n",
      "Decoder Output Shape: torch.Size([1, 39, 8])\n",
      "Linear Layer Output Shape: torch.Size([1, 39, 50257])\n",
      "Input Tokens Shape: torch.Size([1, 40])\n",
      "Max Sequence Length: 40\n",
      "Token Embeddings Shape: torch.Size([1, 40, 8])\n",
      "Token Position Encodings Shape: torch.Size([40, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([1, 40, 8])\n",
      "Decoder Output Shape: torch.Size([1, 40, 8])\n",
      "Linear Layer Output Shape: torch.Size([1, 40, 50257])\n",
      "Input Tokens Shape: torch.Size([1, 41])\n",
      "Max Sequence Length: 41\n",
      "Token Embeddings Shape: torch.Size([1, 41, 8])\n",
      "Token Position Encodings Shape: torch.Size([41, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([1, 41, 8])\n",
      "Decoder Output Shape: torch.Size([1, 41, 8])\n",
      "Linear Layer Output Shape: torch.Size([1, 41, 50257])\n",
      "Input Tokens Shape: torch.Size([1, 42])\n",
      "Max Sequence Length: 42\n",
      "Token Embeddings Shape: torch.Size([1, 42, 8])\n",
      "Token Position Encodings Shape: torch.Size([42, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([1, 42, 8])\n",
      "Decoder Output Shape: torch.Size([1, 42, 8])\n",
      "Linear Layer Output Shape: torch.Size([1, 42, 50257])\n",
      "Input Tokens Shape: torch.Size([1, 43])\n",
      "Max Sequence Length: 43\n",
      "Token Embeddings Shape: torch.Size([1, 43, 8])\n",
      "Token Position Encodings Shape: torch.Size([43, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([1, 43, 8])\n",
      "Decoder Output Shape: torch.Size([1, 43, 8])\n",
      "Linear Layer Output Shape: torch.Size([1, 43, 50257])\n",
      "Input Tokens Shape: torch.Size([1, 44])\n",
      "Max Sequence Length: 44\n",
      "Token Embeddings Shape: torch.Size([1, 44, 8])\n",
      "Token Position Encodings Shape: torch.Size([44, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([1, 44, 8])\n",
      "Decoder Output Shape: torch.Size([1, 44, 8])\n",
      "Linear Layer Output Shape: torch.Size([1, 44, 50257])\n",
      "Input Tokens Shape: torch.Size([1, 45])\n",
      "Max Sequence Length: 45\n",
      "Token Embeddings Shape: torch.Size([1, 45, 8])\n",
      "Token Position Encodings Shape: torch.Size([45, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([1, 45, 8])\n",
      "Decoder Output Shape: torch.Size([1, 45, 8])\n",
      "Linear Layer Output Shape: torch.Size([1, 45, 50257])\n",
      "Input Tokens Shape: torch.Size([1, 46])\n",
      "Max Sequence Length: 46\n",
      "Token Embeddings Shape: torch.Size([1, 46, 8])\n",
      "Token Position Encodings Shape: torch.Size([46, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([1, 46, 8])\n",
      "Decoder Output Shape: torch.Size([1, 46, 8])\n",
      "Linear Layer Output Shape: torch.Size([1, 46, 50257])\n",
      "Input Tokens Shape: torch.Size([1, 47])\n",
      "Max Sequence Length: 47\n",
      "Token Embeddings Shape: torch.Size([1, 47, 8])\n",
      "Token Position Encodings Shape: torch.Size([47, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([1, 47, 8])\n",
      "Decoder Output Shape: torch.Size([1, 47, 8])\n",
      "Linear Layer Output Shape: torch.Size([1, 47, 50257])\n",
      "Input Tokens Shape: torch.Size([1, 48])\n",
      "Max Sequence Length: 48\n",
      "Token Embeddings Shape: torch.Size([1, 48, 8])\n",
      "Token Position Encodings Shape: torch.Size([48, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([1, 48, 8])\n",
      "Decoder Output Shape: torch.Size([1, 48, 8])\n",
      "Linear Layer Output Shape: torch.Size([1, 48, 50257])\n",
      "Input Tokens Shape: torch.Size([1, 49])\n",
      "Max Sequence Length: 49\n",
      "Token Embeddings Shape: torch.Size([1, 49, 8])\n",
      "Token Position Encodings Shape: torch.Size([49, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([1, 49, 8])\n",
      "Decoder Output Shape: torch.Size([1, 49, 8])\n",
      "Linear Layer Output Shape: torch.Size([1, 49, 50257])\n",
      "Input Tokens Shape: torch.Size([1, 50])\n",
      "Max Sequence Length: 50\n",
      "Token Embeddings Shape: torch.Size([1, 50, 8])\n",
      "Token Position Encodings Shape: torch.Size([50, 8])\n",
      "Token Embeddings with Position Shape: torch.Size([1, 50, 8])\n",
      "Decoder Output Shape: torch.Size([1, 50, 8])\n",
      "Linear Layer Output Shape: torch.Size([1, 50, 50257])\n",
      "Generated sequence: once upon a time, there was a were..........................................\n"
     ]
    }
   ],
   "source": [
    "def inference(transformer, tokenizer, starting_word, max_length, temperature=1.0):\n",
    "    \n",
    "    transformer.eval()\n",
    "    # Convert starting and ending words to token IDs\n",
    "    starting_token_ids = tokenizer.encode(starting_word)\n",
    "\n",
    "    # Convert token IDs to tensor\n",
    "    input_tensor = torch.tensor(starting_token_ids).unsqueeze(0).to(get_device())\n",
    "\n",
    "    # Generate tokens until ending word is reached or maximum length is reached\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            # Forward pass through the model\n",
    "            output = transformer(input_tensor)\n",
    "\n",
    "            # Apply temperature scaling to the logits\n",
    "            scaled_output = output / temperature\n",
    "\n",
    "            # Get the last predicted token\n",
    "            last_token = scaled_output.argmax(dim=-1)[:, -1]\n",
    "\n",
    "            # Append the last token to the input tensor\n",
    "            last_token = last_token.unsqueeze(0).to(input_tensor.device)  # Ensure last_token is on the same device as input_tensor\n",
    "            input_tensor = torch.cat([input_tensor, last_token], dim=-1)\n",
    "\n",
    "            # Check if the ending word is reached\n",
    "            if (last_token == tokenizer.eos_token_id):\n",
    "                break\n",
    "\n",
    "    # Decode the generated tokens\n",
    "    generated_text = tokenizer.decode(input_tensor.squeeze().tolist())\n",
    "\n",
    "    return generated_text\n",
    "\n",
    "# Define the path where the model is saved\n",
    "model_path = '/workspace/transformer.pth'\n",
    "\n",
    "# Load the model from the saved file\n",
    "transformer.load_state_dict(torch.load(model_path))\n",
    "\n",
    "# Now you can use the inference function with the loaded and evaluated model\n",
    "starting_word = \"once upon a time, there was a\"\n",
    "max_length = 43\n",
    "temperature = 0.8\n",
    "\n",
    "generated_sequence = inference(transformer, tokenizer, starting_word\n",
    "                               , max_length, temperature)\n",
    "print(\"Generated sequence:\", generated_sequence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76652cc4-b9f7-4951-9962-43851e53f95d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
