{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e257275-c60b-45f1-9da6-9150213db888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /Users/beesamprajveenkumar/anaconda3/lib/python3.11/site-packages (1.26.4)\n",
      "Requirement already satisfied: pandas in /Users/beesamprajveenkumar/anaconda3/lib/python3.11/site-packages (2.0.3)\n",
      "Requirement already satisfied: datasets in /Users/beesamprajveenkumar/anaconda3/lib/python3.11/site-packages (2.18.0)\n",
      "Requirement already satisfied: sentencepiece in /Users/beesamprajveenkumar/anaconda3/lib/python3.11/site-packages (0.2.0)\n",
      "Requirement already satisfied: torch in /Users/beesamprajveenkumar/anaconda3/lib/python3.11/site-packages (2.2.1)\n",
      "Requirement already satisfied: matplotlib in /Users/beesamprajveenkumar/anaconda3/lib/python3.11/site-packages (3.8.2)\n",
      "Requirement already satisfied: transformers in /Users/beesamprajveenkumar/anaconda3/lib/python3.11/site-packages (4.38.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/beesamprajveenkumar/anaconda3/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/beesamprajveenkumar/anaconda3/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/beesamprajveenkumar/anaconda3/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: filelock in /Users/beesamprajveenkumar/anaconda3/lib/python3.11/site-packages (from datasets) (3.13.3)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /Users/beesamprajveenkumar/anaconda3/lib/python3.11/site-packages (from datasets) (15.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in /Users/beesamprajveenkumar/anaconda3/lib/python3.11/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/beesamprajveenkumar/anaconda3/lib/python3.11/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/beesamprajveenkumar/anaconda3/lib/python3.11/site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /Users/beesamprajveenkumar/anaconda3/lib/python3.11/site-packages (from datasets) (4.65.0)\n",
      "Requirement already satisfied: xxhash in /Users/beesamprajveenkumar/anaconda3/lib/python3.11/site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: multiprocess in /Users/beesamprajveenkumar/anaconda3/lib/python3.11/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /Users/beesamprajveenkumar/anaconda3/lib/python3.11/site-packages (from datasets) (2024.2.0)\n",
      "Requirement already satisfied: aiohttp in /Users/beesamprajveenkumar/anaconda3/lib/python3.11/site-packages (from datasets) (3.8.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in /Users/beesamprajveenkumar/anaconda3/lib/python3.11/site-packages (from datasets) (0.21.4)\n",
      "Requirement already satisfied: packaging in /Users/beesamprajveenkumar/anaconda3/lib/python3.11/site-packages (from datasets) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/beesamprajveenkumar/anaconda3/lib/python3.11/site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/beesamprajveenkumar/anaconda3/lib/python3.11/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in /Users/beesamprajveenkumar/anaconda3/lib/python3.11/site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in /Users/beesamprajveenkumar/anaconda3/lib/python3.11/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/beesamprajveenkumar/anaconda3/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/beesamprajveenkumar/anaconda3/lib/python3.11/site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/beesamprajveenkumar/anaconda3/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/beesamprajveenkumar/anaconda3/lib/python3.11/site-packages (from matplotlib) (4.47.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/beesamprajveenkumar/anaconda3/lib/python3.11/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: pillow>=8 in /Users/beesamprajveenkumar/anaconda3/lib/python3.11/site-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/beesamprajveenkumar/anaconda3/lib/python3.11/site-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/beesamprajveenkumar/anaconda3/lib/python3.11/site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Users/beesamprajveenkumar/anaconda3/lib/python3.11/site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/beesamprajveenkumar/anaconda3/lib/python3.11/site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/beesamprajveenkumar/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/beesamprajveenkumar/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (3.3.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/beesamprajveenkumar/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/beesamprajveenkumar/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/beesamprajveenkumar/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/beesamprajveenkumar/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/beesamprajveenkumar/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/beesamprajveenkumar/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/beesamprajveenkumar/anaconda3/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/beesamprajveenkumar/anaconda3/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/beesamprajveenkumar/anaconda3/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/beesamprajveenkumar/anaconda3/lib/python3.11/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/beesamprajveenkumar/anaconda3/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy pandas datasets sentencepiece torch matplotlib transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d97b45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import sentencepiece as spm\n",
    "from PIL import Image\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "from io import BytesIO\n",
    "import io\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import math\n",
    "from torch import nn\n",
    "from datasets import load_dataset\n",
    "from transformers import GPT2Tokenizer, GPT2TokenizerFast\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3477e81c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"roneneldan/TinyStories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a1341c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 2119719\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 21990\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f4d5e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = dataset['train']\n",
    "validation_data = dataset['validation']\n",
    "\n",
    "train_df = train_data.to_pandas()\n",
    "validation_df = validation_data.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72cae17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()\n",
    "train_df = train_df[:4000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7b546e",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ad5038b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained GPT-2 tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55dbcdc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 50257\n"
     ]
    }
   ],
   "source": [
    "# Get the vocabulary size\n",
    "vocab_size = len(tokenizer)\n",
    "print(\"Vocabulary size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79bd18f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1106 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "#tokenizing the ids of the captions and including it into df\n",
    "train_df['text_tokens_ids'] = train_df['text'].apply(lambda x: tokenizer.encode(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "856dd4ea-1189-45a5-9617-a9b554ce2d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4000 entries, 0 to 3999\n",
      "Data columns (total 2 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   text             4000 non-null   object\n",
      " 1   text_tokens_ids  4000 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 62.6+ KB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7ca6541",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_tokens_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One day, a little girl named Lily found a need...</td>\n",
       "      <td>[3198, 1110, 11, 257, 1310, 2576, 3706, 20037,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Once upon a time, there was a little car named...</td>\n",
       "      <td>[7454, 2402, 257, 640, 11, 612, 373, 257, 1310...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>One day, a little fish named Fin was swimming ...</td>\n",
       "      <td>[3198, 1110, 11, 257, 1310, 5916, 3706, 4463, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Once upon a time, in a land full of trees, the...</td>\n",
       "      <td>[7454, 2402, 257, 640, 11, 287, 257, 1956, 133...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Once upon a time, there was a little girl name...</td>\n",
       "      <td>[7454, 2402, 257, 640, 11, 612, 373, 257, 1310...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  One day, a little girl named Lily found a need...   \n",
       "1  Once upon a time, there was a little car named...   \n",
       "2  One day, a little fish named Fin was swimming ...   \n",
       "3  Once upon a time, in a land full of trees, the...   \n",
       "4  Once upon a time, there was a little girl name...   \n",
       "\n",
       "                                     text_tokens_ids  \n",
       "0  [3198, 1110, 11, 257, 1310, 2576, 3706, 20037,...  \n",
       "1  [7454, 2402, 257, 640, 11, 612, 373, 257, 1310...  \n",
       "2  [3198, 1110, 11, 257, 1310, 5916, 3706, 4463, ...  \n",
       "3  [7454, 2402, 257, 640, 11, 287, 257, 1956, 133...  \n",
       "4  [7454, 2402, 257, 640, 11, 612, 373, 257, 1310...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc83fe68",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "dcc14044",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset class\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, df, max_seq_len=20):\n",
    "        self.df = df\n",
    "        self.max_seq_len = max_seq_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        token_ids = self.df['text_tokens_ids'][idx]\n",
    "        print(\"Input sentence is: \", self.df['text'][idx]) \n",
    "\n",
    "        # #Preprocess the captions for gpt-2\n",
    "        SOS = tokenizer.bos_token_id\n",
    "        EOS = tokenizer.eos_token_id\n",
    "        \n",
    "        input_text = token_ids.copy()\n",
    "        input_text.insert(0, SOS)\n",
    "\n",
    "        target_text = token_ids.copy()\n",
    "        target_text.append(EOS)\n",
    "        \n",
    "        cap_len = len(input_text)\n",
    "        pad_len = self.max_seq_len - cap_len\n",
    "        mask = []\n",
    "\n",
    "\n",
    "        if pad_len > 0:\n",
    "            zero_pad = [0] * pad_len\n",
    "            input_text.extend(zero_pad)\n",
    "            input_text_padded = input_text\n",
    "            \n",
    "            target_text.extend(zero_pad)\n",
    "            target_text_padded = target_text\n",
    "\n",
    "            mask.extend([1] * cap_len)\n",
    "            mask.extend([0] * pad_len)\n",
    "        else:\n",
    "            input_text_padded = input_text[:self.max_seq_len]\n",
    "            target_text_padded = target_text[:self.max_seq_len]\n",
    "            mask.extend([1] * self.max_seq_len)\n",
    "\n",
    "        return {\n",
    "            'input_tokens' : torch.tensor(input_text_padded),\n",
    "            'target_tokens': torch.tensor(target_text_padded),\n",
    "            'mask'         : torch.tensor(mask)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93479562",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5c87dfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    return torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "def scaled_dot_product(q, k, v, pad_mask=None, atn_mask=False):\n",
    "    d_k = q.size()[-1]\n",
    "    \n",
    "    # Move q, k, and v tensors to the same device\n",
    "    q, k, v = q.to(get_device()), k.to(get_device()), v.to(get_device())\n",
    "    \n",
    "    scaled = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(d_k)\n",
    "    if atn_mask:\n",
    "        dia_mask = torch.full(scaled.size(), float('-inf'), device=get_device())\n",
    "        dia_mask = torch.triu(dia_mask, diagonal=1)\n",
    "        scaled += dia_mask\n",
    "    attention = F.softmax(scaled, dim=-1)\n",
    "    if pad_mask is not None:\n",
    "        pad_mask = pad_mask.unsqueeze(1).unsqueeze(1) * pad_mask.unsqueeze(1).unsqueeze(3)\n",
    "        # Move pad_mask to the same device\n",
    "        pad_mask = pad_mask.to(get_device())\n",
    "        attention = attention.masked_fill(pad_mask==0, 0)\n",
    "    values = torch.matmul(attention, v)\n",
    "    return values, attention\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_sequence_length):\n",
    "        super().__init__()\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, device=torch.device('cpu')):  # Pass device as an argument\n",
    "        even_i = torch.arange(0, self.d_model, 2).float().to(device)  # Move tensor to device\n",
    "        denominator = torch.pow(10000, even_i / self.d_model)\n",
    "        position = torch.arange(self.max_sequence_length, device=device).reshape(self.max_sequence_length, 1)  # Move tensor to device\n",
    "        even_PE = torch.sin(position / denominator)\n",
    "        odd_PE = torch.cos(position / denominator)\n",
    "        stacked = torch.stack([even_PE, odd_PE], dim=2)\n",
    "        PE = torch.flatten(stacked, start_dim=1, end_dim=2)\n",
    "        return PE\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.qkv_layer = nn.Linear(d_model , 3 * d_model)\n",
    "        self.linear_layer = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, x, pad_mask=None, atn_mask=False):\n",
    "        batch_size, sequence_length, d_model = x.shape\n",
    "        qkv = self.qkv_layer(x)\n",
    "        qkv = qkv.reshape(batch_size, sequence_length, self.num_heads, 3 * self.head_dim)\n",
    "        qkv = qkv.permute(0, 2, 1, 3)\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "        values, attention = scaled_dot_product(q, k, v, pad_mask, atn_mask = atn_mask)\n",
    "        values = values.reshape(batch_size, sequence_length, self.num_heads * self.head_dim)\n",
    "        out = self.linear_layer(values)\n",
    "        return out\n",
    "\n",
    "\n",
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, parameters_shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.parameters_shape=parameters_shape\n",
    "        self.eps=eps\n",
    "        self.gamma = nn.Parameter(torch.ones(parameters_shape))\n",
    "        self.beta =  nn.Parameter(torch.zeros(parameters_shape))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        dims = [-(i + 1) for i in range(len(self.parameters_shape))]\n",
    "        mean = inputs.mean(dim=dims, keepdim=True)\n",
    "        var = ((inputs - mean) ** 2).mean(dim=dims, keepdim=True)\n",
    "        std = (var + self.eps).sqrt()\n",
    "        y = (inputs - mean) / std\n",
    "        out = self.gamma * y + self.beta\n",
    "        return out\n",
    "\n",
    "  \n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, hidden, drop_prob=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, hidden)\n",
    "        self.linear2 = nn.Linear(hidden, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attention1 = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(p=drop_prob)\n",
    "        self.self_attention2 = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout2 = nn.Dropout(p=drop_prob)\n",
    "        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout3 = nn.Dropout(p=drop_prob)\n",
    "    \n",
    "    # Override the forward method to handle None values for pad_mask\n",
    "    def forward(self, y, pad_mask, atn_mask):\n",
    "        _y = y\n",
    "        \n",
    "        # Check if pad_mask is None before attempting to move it to device\n",
    "        if pad_mask is not None:\n",
    "            pad_mask = pad_mask.to(get_device())\n",
    "        \n",
    "        y = self.self_attention1(y, pad_mask, atn_mask)\n",
    "        y = self.dropout1(y) \n",
    "        y = self.norm1(y + _y) \n",
    "        _y = y\n",
    "        \n",
    "        y = self.ffn(y) \n",
    "        y = self.dropout3(y) \n",
    "        y = self.norm3(y + _y) \n",
    "        return y\n",
    "\n",
    "class SequentialDecoder(nn.Sequential):\n",
    "    def forward(self, *inputs):\n",
    "        y, pad_mask, atn_mask = inputs\n",
    "        for module in self._modules.values():\n",
    "            y = module(y, pad_mask, atn_mask)\n",
    "        return y\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 d_model, \n",
    "                 ffn_hidden, \n",
    "                 num_heads, \n",
    "                 drop_prob, \n",
    "                 num_layers):\n",
    "        super().__init__()\n",
    "        self.layers = SequentialDecoder(*[DecoderLayer(d_model, ffn_hidden, num_heads, drop_prob) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, y, pad_mask = None, atn_mask = True):\n",
    "        y = self.layers(y, pad_mask, atn_mask)\n",
    "        return y\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, \n",
    "                d_model, \n",
    "                ffn_hidden, \n",
    "                num_heads, \n",
    "                drop_prob, \n",
    "                num_layers,\n",
    "                vocab_size               \n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.dec_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.dec_pos_encoding = PositionalEncoding(d_model, 1)\n",
    "        \n",
    "        self.decoder = Decoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers)\n",
    "        self.linear = nn.Linear(d_model, vocab_size)\n",
    "        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "    def forward(self, input_tokens, pad_mask=None, atn_mask=True):\n",
    "        print(\"--------------------------Step 1---------------------------------\")\n",
    "        print(\"-----------------------------------------------------------------\")\n",
    "        print(\"Transformer received inputs\")\n",
    "        print(\"Input Tokens Shape:\", input_tokens.shape)\n",
    "        print(\"Input Tokens Shape:\", input_tokens)\n",
    "\n",
    "        # Move input tensors to the appropriate device\n",
    "        input_tokens = input_tokens.to(self.device)\n",
    "        pad_mask = pad_mask.to(self.device) if pad_mask is not None else None\n",
    "        max_sequence_length = input_tokens.shape[1]  # Fix this line to get the correct sequence length\n",
    "        print(\"Max Sequence Length:\", max_sequence_length)\n",
    "        print(\"--------------------------Step 2---------------------------------\")\n",
    "        print(\"-----------------------------------------------------------------\")\n",
    "        print(\"Preparation of the embeddings to the inputs we have recieved\")\n",
    "        # Compute token embeddings\n",
    "        token_embeddings = self.dec_embedding(input_tokens) \n",
    "        print(\"Token Embeddings Shape:\", token_embeddings.shape)\n",
    "        print(\"Token Embeddings:\", token_embeddings)\n",
    "\n",
    "        self.dec_pos_encoding = PositionalEncoding(self.d_model, max_sequence_length)\n",
    "        token_pos_encodings = self.dec_pos_encoding(device=self.device)  # Pass device argument\n",
    "        print(\"Token Position Encodings Shape:\", token_pos_encodings.shape)\n",
    "\n",
    "        token_embeddings_with_pos = token_embeddings + token_pos_encodings.unsqueeze(0)\n",
    "        print(\"Token Embeddings with Position Shape:\", token_embeddings_with_pos.shape)\n",
    "        print(\"Token Embeddings with Position:\", token_embeddings_with_pos)\n",
    "\n",
    "        # Perform the rest of the forward pass\n",
    "        print(\"--------------------------Step 3---------------------------------\")\n",
    "        print(\"-----------------------------------------------------------------\")\n",
    "        print(\"Passing the embeddings with the positions to the decoder\")\n",
    "        out = self.decoder(token_embeddings_with_pos, pad_mask, atn_mask)\n",
    "        print(\"Decoder Output Shape:\", out.shape)\n",
    "        print(\"Decoder Output:\", out)\n",
    "\n",
    "        print(\"--------------------------Step 4---------------------------------\")\n",
    "        print(\"-----------------------------------------------------------------\")\n",
    "        print(\"Passing the decoder output to the feedforward layer\")\n",
    "\n",
    "        out = self.linear(out)\n",
    "        print(\"Linear Layer Output Shape:\", out.shape)\n",
    "        print(\"Linear Layer Output:\", out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "29b0d8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 4\n",
    "num_heads = 1\n",
    "drop_prob = 0.1\n",
    "batch_size = 1\n",
    "ffn_hidden = 2\n",
    "num_layers = 2\n",
    "vocab_size = vocab_size\n",
    "num_epochs = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d41ffa72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to get the device\n",
    "def get_device():\n",
    "    return torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "257d13af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (dec_embedding): Embedding(50257, 4)\n",
       "  (dec_pos_encoding): PositionalEncoding()\n",
       "  (decoder): Decoder(\n",
       "    (layers): SequentialDecoder(\n",
       "      (0): DecoderLayer(\n",
       "        (self_attention1): MultiHeadAttention(\n",
       "          (qkv_layer): Linear(in_features=4, out_features=12, bias=True)\n",
       "          (linear_layer): Linear(in_features=4, out_features=4, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (self_attention2): MultiHeadAttention(\n",
       "          (qkv_layer): Linear(in_features=4, out_features=12, bias=True)\n",
       "          (linear_layer): Linear(in_features=4, out_features=4, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (ffn): PositionwiseFeedForward(\n",
       "          (linear1): Linear(in_features=4, out_features=2, bias=True)\n",
       "          (linear2): Linear(in_features=2, out_features=4, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (norm3): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): DecoderLayer(\n",
       "        (self_attention1): MultiHeadAttention(\n",
       "          (qkv_layer): Linear(in_features=4, out_features=12, bias=True)\n",
       "          (linear_layer): Linear(in_features=4, out_features=4, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (self_attention2): MultiHeadAttention(\n",
       "          (qkv_layer): Linear(in_features=4, out_features=12, bias=True)\n",
       "          (linear_layer): Linear(in_features=4, out_features=4, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (ffn): PositionwiseFeedForward(\n",
       "          (linear1): Linear(in_features=4, out_features=2, bias=True)\n",
       "          (linear2): Linear(in_features=2, out_features=4, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (norm3): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=4, out_features=50257, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Move the model to the appropriate device\n",
    "transformer = Transformer(d_model, ffn_hidden, num_heads, drop_prob, num_layers, vocab_size)\n",
    "transformer.to(get_device())  # Move the model to GPU if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "475ef085",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MyDataset(train_df[:1], max_seq_len=10)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "538572da-14a2-477e-a906-548be94bd682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 452,725 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(transformer):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "50656624",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sentence is:  One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt.\n",
      "\n",
      "Lily went to her mom and said, \"Mom, I found this needle. Can you share it with me and sew my shirt?\" Her mom smiled and said, \"Yes, Lily, we can share the needle and fix your shirt.\"\n",
      "\n",
      "Together, they shared the needle and sewed the button on Lily's shirt. It was not difficult for them because they were sharing and helping each other. After they finished, Lily thanked her mom for sharing the needle and fixing her shirt. They both felt happy because they had shared and worked together.\n",
      "--------------------------Step 1---------------------------------\n",
      "-----------------------------------------------------------------\n",
      "Transformer received inputs\n",
      "Input Tokens Shape: torch.Size([1, 10])\n",
      "Input Tokens Shape: tensor([[50256,  3198,  1110,    11,   257,  1310,  2576,  3706, 20037,  1043]])\n",
      "Max Sequence Length: 10\n",
      "--------------------------Step 2---------------------------------\n",
      "-----------------------------------------------------------------\n",
      "Preparation of the embeddings to the inputs we have recieved\n",
      "Token Embeddings Shape: torch.Size([1, 10, 4])\n",
      "Token Embeddings: tensor([[[-1.3686,  0.3235,  1.2828, -1.1527],\n",
      "         [ 0.8705, -0.0495,  0.8773, -2.1956],\n",
      "         [-1.2403,  1.1167, -0.1723,  0.3796],\n",
      "         [ 0.5754, -0.8282, -0.4794,  1.1129],\n",
      "         [ 0.1281, -0.2372,  0.6704, -0.1213],\n",
      "         [ 1.2138, -0.1216,  0.3244,  0.2540],\n",
      "         [ 0.3361, -0.6369, -1.0199, -0.4877],\n",
      "         [ 0.1917,  0.2069, -0.9960,  0.2806],\n",
      "         [ 0.0484,  0.7026, -1.1630, -1.2672],\n",
      "         [-1.9664, -1.1313,  0.1440, -0.7516]]], grad_fn=<EmbeddingBackward0>)\n",
      "Token Position Encodings Shape: torch.Size([10, 4])\n",
      "Token Embeddings with Position Shape: torch.Size([1, 10, 4])\n",
      "Token Embeddings with Position: tensor([[[-1.3686,  1.3235,  1.2828, -0.1527],\n",
      "         [ 1.7120,  0.4908,  0.8873, -1.1956],\n",
      "         [-0.3310,  0.7005, -0.1523,  1.3794],\n",
      "         [ 0.7165, -1.8181, -0.4494,  2.1124],\n",
      "         [-0.6287, -0.8909,  0.7104,  0.8779],\n",
      "         [ 0.2548,  0.1620,  0.3744,  1.2527],\n",
      "         [ 0.0567,  0.3233, -0.9600,  0.5105],\n",
      "         [ 0.8487,  0.9608, -0.9261,  1.2781],\n",
      "         [ 1.0377,  0.5571, -1.0831, -0.2704],\n",
      "         [-1.5542, -2.0424,  0.2339,  0.2443]]], grad_fn=<AddBackward0>)\n",
      "--------------------------Step 3---------------------------------\n",
      "-----------------------------------------------------------------\n",
      "Passing the embeddings with the positions to the decoder\n",
      "Decoder Output Shape: torch.Size([1, 10, 4])\n",
      "Decoder Output: tensor([[[-0.8688,  0.8442,  1.1375, -1.1129],\n",
      "         [ 1.1864, -0.1916,  0.5176, -1.5125],\n",
      "         [-1.2343,  0.6187, -0.6679,  1.2836],\n",
      "         [ 1.0231, -1.2995, -0.6448,  0.9212],\n",
      "         [ 0.0373, -1.6122,  1.0696,  0.5053],\n",
      "         [-1.0080, -0.8172,  0.3424,  1.4828],\n",
      "         [ 0.5894,  0.9829, -1.6377,  0.0654],\n",
      "         [ 1.0998,  0.2287, -1.6275,  0.2990],\n",
      "         [ 1.4844,  0.1742, -1.2713, -0.3873],\n",
      "         [-0.4809, -1.3685,  1.2293,  0.6201]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "--------------------------Step 4---------------------------------\n",
      "-----------------------------------------------------------------\n",
      "Passing the decoder output to the feedforward layer\n",
      "Linear Layer Output Shape: torch.Size([1, 10, 50257])\n",
      "Linear Layer Output: tensor([[[ 1.0550, -0.1192, -0.1457,  ..., -0.6289,  0.2669, -0.6806],\n",
      "         [ 0.0633,  0.4066, -1.0215,  ..., -0.3137,  0.1241,  0.4566],\n",
      "         [-0.8009, -0.7507,  1.1245,  ...,  0.4592, -0.1520, -0.3488],\n",
      "         ...,\n",
      "         [-1.6432, -0.2976,  0.1108,  ...,  0.4936, -0.4516,  0.9694],\n",
      "         [-1.3026, -0.0699, -0.3354,  ...,  0.2489, -0.3723,  1.0485],\n",
      "         [-0.2718,  0.2161, -0.0212,  ...,  0.4734,  0.4799, -0.4149]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Epoch [1/1], Batch [1/1], Loss: 10.83175277709961\n",
      "Epoch [1/1], Total Loss: 10.83175277709961\n"
     ]
    }
   ],
   "source": [
    "# Define the optimizer and scheduler\n",
    "optim = torch.optim.Adam(transformer.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9)\n",
    "#scheduler = StepLR(optim, step_size=20, gamma=0.1)  # Learning rate decreases by a factor of 0.1 every 5 epochs\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Iterate over epochs\n",
    "for epoch in range(num_epochs):\n",
    "    transformer.train()  # Set the model to training mode\n",
    "    total_loss = 0\n",
    "    for batch_num, batch in enumerate(dataloader):\n",
    "        input_tokens, target_tokens, pad_mask = batch['input_tokens'], batch['target_tokens'], batch['mask']\n",
    "        \n",
    "        # Move tensors to GPU if available\n",
    "        input_tokens = input_tokens.to(get_device())\n",
    "        target_tokens = target_tokens.to(get_device())\n",
    "        pad_mask = pad_mask.to(get_device())\n",
    "        \n",
    "        optim.zero_grad()\n",
    "        predictions = transformer(input_tokens, pad_mask)\n",
    "        loss = criterion(predictions.view(-1, vocab_size), target_tokens.view(-1))\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Print loss after each batch\n",
    "        print_every = 50  # Define how often to print the loss\n",
    "        if batch_num % print_every == 0:\n",
    "           print(f'Epoch [{epoch + 1}/{num_epochs}], Batch [{batch_num + 1}/{len(dataloader)}], Loss: {loss.item()}')\n",
    "            \n",
    "    \n",
    "    # Print total loss after each epoch\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Total Loss: {total_loss/len(dataloader)}')\n",
    "    \n",
    "    # Update the learning rate\n",
    "    #scheduler.step()\n",
    "    \n",
    "    # Save the model after each epoch\n",
    "    model_path = 'transformer.pth'\n",
    "    torch.save(transformer.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76652cc4-b9f7-4951-9962-43851e53f95d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928b3d7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
