{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e257275-c60b-45f1-9da6-9150213db888",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install numpy pandas datasets sentencepiece torch matplotlib transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d97b45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "import pandas as pd\n",
    "import sentencepiece as spm\n",
    "from PIL import Image\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "from io import BytesIO\n",
    "import io\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import math\n",
    "from torch import nn\n",
    "from datasets import load_dataset\n",
    "from transformers import GPT2Tokenizer, GPT2TokenizerFast\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3477e81c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"roneneldan/TinyStories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a1341c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 2119719\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 21990\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f4d5e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = dataset['train']\n",
    "validation_data = dataset['validation']\n",
    "\n",
    "train_df = train_data.to_pandas()\n",
    "validation_df = validation_data.to_pandas()\n",
    "df = pd.concat([train_df, validation_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72cae17f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One day, a little girl named Lily found a need...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Once upon a time, there was a little car named...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>One day, a little fish named Fin was swimming ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Once upon a time, in a land full of trees, the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Once upon a time, there was a little girl name...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  One day, a little girl named Lily found a need...\n",
       "1  Once upon a time, there was a little car named...\n",
       "2  One day, a little fish named Fin was swimming ...\n",
       "3  Once upon a time, in a land full of trees, the...\n",
       "4  Once upon a time, there was a little girl name..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[:10000]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7b546e",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ad5038b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained GPT-2 tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "#tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55dbcdc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 50257\n"
     ]
    }
   ],
   "source": [
    "# Get the vocabulary size\n",
    "vocab_size = len(tokenizer)\n",
    "print(\"Vocabulary size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79bd18f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1106 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "#tokenizing the ids of the captions and including it into df\n",
    "df['text_tokens_ids'] = df['text'].apply(lambda x: tokenizer.encode(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7ca6541",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_tokens_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One day, a little girl named Lily found a need...</td>\n",
       "      <td>[3198, 1110, 11, 257, 1310, 2576, 3706, 20037,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Once upon a time, there was a little car named...</td>\n",
       "      <td>[7454, 2402, 257, 640, 11, 612, 373, 257, 1310...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>One day, a little fish named Fin was swimming ...</td>\n",
       "      <td>[3198, 1110, 11, 257, 1310, 5916, 3706, 4463, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Once upon a time, in a land full of trees, the...</td>\n",
       "      <td>[7454, 2402, 257, 640, 11, 287, 257, 1956, 133...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Once upon a time, there was a little girl name...</td>\n",
       "      <td>[7454, 2402, 257, 640, 11, 612, 373, 257, 1310...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  One day, a little girl named Lily found a need...   \n",
       "1  Once upon a time, there was a little car named...   \n",
       "2  One day, a little fish named Fin was swimming ...   \n",
       "3  Once upon a time, in a land full of trees, the...   \n",
       "4  Once upon a time, there was a little girl name...   \n",
       "\n",
       "                                     text_tokens_ids  \n",
       "0  [3198, 1110, 11, 257, 1310, 2576, 3706, 20037,...  \n",
       "1  [7454, 2402, 257, 640, 11, 612, 373, 257, 1310...  \n",
       "2  [3198, 1110, 11, 257, 1310, 5916, 3706, 4463, ...  \n",
       "3  [7454, 2402, 257, 640, 11, 287, 257, 1956, 133...  \n",
       "4  [7454, 2402, 257, 640, 11, 612, 373, 257, 1310...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc83fe68",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dcc14044",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset class\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, df, max_seq_len=20):\n",
    "        self.df = df\n",
    "        self.max_seq_len = max_seq_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        token_ids = self.df['text_tokens_ids'][idx] \n",
    "\n",
    "        # #Preprocess the captions\n",
    "        SOS = tokenizer.bos_token_id\n",
    "        EOS = tokenizer.eos_token_id\n",
    "\n",
    "        #SOS = tokenizer.cls_token_id\n",
    "        #EOS = tokenizer.sep_token_id\n",
    "        \n",
    "\n",
    "          # # Define custom BOS and EOS tokens\n",
    "        # BOS_TOKEN = \"<s>\"\n",
    "        # EOS_TOKEN = \"</s>\"\n",
    "        \n",
    "        # # Get token IDs for BOS and EOS tokens\n",
    "        # SOS = tokenizer.PieceToId(BOS_TOKEN)\n",
    "        # EOS = tokenizer.PieceToId(EOS_TOKEN)\n",
    "        \n",
    "\n",
    "        \n",
    "        input_text = token_ids.copy()\n",
    "        input_text.insert(0, SOS)\n",
    "\n",
    "        target_text = token_ids.copy()\n",
    "        target_text.append(SOS)\n",
    "        \n",
    "        cap_len = len(input_text)\n",
    "        pad_len = self.max_seq_len - cap_len\n",
    "        mask = []\n",
    "\n",
    "\n",
    "        if pad_len > 0:\n",
    "            zero_pad = [0] * pad_len\n",
    "            input_text.extend(zero_pad)\n",
    "            input_text_padded = input_text\n",
    "            \n",
    "            target_text.extend(zero_pad)\n",
    "            target_text_padded = target_text\n",
    "\n",
    "            mask.extend([1] * cap_len)\n",
    "            mask.extend([0] * pad_len)\n",
    "        else:\n",
    "            input_text_padded = input_text[:self.max_seq_len]\n",
    "            target_text_padded = target_text[:self.max_seq_len]\n",
    "            mask.extend([1] * self.max_seq_len)\n",
    "\n",
    "        return {\n",
    "            'input_tokens' : torch.tensor(input_text_padded),\n",
    "            'target_tokens': torch.tensor(target_text_padded),\n",
    "            'mask'         : torch.tensor(mask)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93479562",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c87dfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    return torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "def scaled_dot_product(q, k, v, pad_mask=None, atn_mask=False):\n",
    "    d_k = q.size()[-1]\n",
    "    \n",
    "    # Move q, k, and v tensors to the same device\n",
    "    q, k, v = q.to(get_device()), k.to(get_device()), v.to(get_device())\n",
    "    \n",
    "    scaled = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(d_k)\n",
    "    if atn_mask:\n",
    "        dia_mask = torch.full(scaled.size(), float('-inf'), device=get_device())\n",
    "        dia_mask = torch.triu(dia_mask, diagonal=1)\n",
    "        scaled += dia_mask\n",
    "    attention = F.softmax(scaled, dim=-1)\n",
    "    if pad_mask is not None:\n",
    "        pad_mask = pad_mask.unsqueeze(1).unsqueeze(1) * pad_mask.unsqueeze(1).unsqueeze(3)\n",
    "        # Move pad_mask to the same device\n",
    "        pad_mask = pad_mask.to(get_device())\n",
    "        attention = attention.masked_fill(pad_mask==0, 0)\n",
    "    values = torch.matmul(attention, v)\n",
    "    return values, attention\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_sequence_length):\n",
    "        super().__init__()\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, device=torch.device('cpu')):  # Pass device as an argument\n",
    "        even_i = torch.arange(0, self.d_model, 2).float().to(device)  # Move tensor to device\n",
    "        denominator = torch.pow(10000, even_i / self.d_model)\n",
    "        position = torch.arange(self.max_sequence_length, device=device).reshape(self.max_sequence_length, 1)  # Move tensor to device\n",
    "        even_PE = torch.sin(position / denominator)\n",
    "        odd_PE = torch.cos(position / denominator)\n",
    "        stacked = torch.stack([even_PE, odd_PE], dim=2)\n",
    "        PE = torch.flatten(stacked, start_dim=1, end_dim=2)\n",
    "        return PE\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.qkv_layer = nn.Linear(d_model , 3 * d_model)\n",
    "        self.linear_layer = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, x, pad_mask=None, atn_mask=False):\n",
    "        batch_size, sequence_length, d_model = x.shape\n",
    "        qkv = self.qkv_layer(x)\n",
    "        qkv = qkv.reshape(batch_size, sequence_length, self.num_heads, 3 * self.head_dim)\n",
    "        qkv = qkv.permute(0, 2, 1, 3)\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "        values, attention = scaled_dot_product(q, k, v, pad_mask, atn_mask = atn_mask)\n",
    "        values = values.reshape(batch_size, sequence_length, self.num_heads * self.head_dim)\n",
    "        out = self.linear_layer(values)\n",
    "        return out\n",
    "\n",
    "  \n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, hidden, drop_prob=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, hidden)\n",
    "        self.linear2 = nn.Linear(hidden, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        # self.norm0 = nn.LayerNorm(d_model)\n",
    "        self.self_attention1 = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(p=drop_prob)\n",
    "        self.self_attention2 = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout2 = nn.Dropout(p=drop_prob)\n",
    "        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout3 = nn.Dropout(p=drop_prob)\n",
    "    \n",
    "    # Override the forward method to handle None values for pad_mask\n",
    "    def forward(self, y, pad_mask, atn_mask):\n",
    "        _y = y\n",
    "        \n",
    "        # Check if pad_mask is None before attempting to move it to device\n",
    "        if pad_mask is not None:\n",
    "            pad_mask = pad_mask.to(get_device())\n",
    "        \n",
    "        # y = self.norm0(y)\n",
    "        y = self.self_attention1(y, pad_mask, atn_mask)\n",
    "        y = self.dropout1(y) \n",
    "        y = self.norm1(y + _y) \n",
    "        _y = y\n",
    "        \n",
    "        # Check if pad_mask is None before attempting to move it to device\n",
    "        # if pad_mask is not None:\n",
    "        #     pad_mask = pad_mask.to(get_device())\n",
    "        \n",
    "        #y = self.self_attention2(y)         \n",
    "        #y = self.dropout2(y)\n",
    "        #y = self.norm2(y + _y)  \n",
    "\n",
    "        #_y = y  \n",
    "        y = self.ffn(y) \n",
    "        y = self.dropout3(y) \n",
    "        y = self.norm3(y + _y) \n",
    "        return y\n",
    "\n",
    "\n",
    "\n",
    "class SequentialDecoder(nn.Sequential):\n",
    "    def forward(self, *inputs):\n",
    "        y, pad_mask, atn_mask = inputs\n",
    "        for module in self._modules.values():\n",
    "            y = module(y, pad_mask, atn_mask) #30 x 200 x 512\n",
    "        return y\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 d_model, \n",
    "                 ffn_hidden, \n",
    "                 num_heads, \n",
    "                 drop_prob, \n",
    "                 num_layers):\n",
    "        super().__init__()\n",
    "        self.layers = SequentialDecoder(*[DecoderLayer(d_model, ffn_hidden, num_heads, drop_prob) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, y, pad_mask = None, atn_mask = True):\n",
    "        y = self.layers(y, pad_mask, atn_mask)\n",
    "        return y\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2Model\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, \n",
    "                d_model, \n",
    "                ffn_hidden, \n",
    "                num_heads, \n",
    "                drop_prob, \n",
    "                num_layers,\n",
    "                vocab_size,\n",
    "                freeze_gpt2=True          # Whether to freeze the weights of the GPT-2 model\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # Load the pre-trained GPT-2 model\n",
    "        self.gpt2 = GPT2Model.from_pretrained('gpt2')\n",
    "        \n",
    "        # Freeze the weights of the GPT-2 model if specified\n",
    "        if freeze_gpt2:\n",
    "            for param in self.gpt2.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        # Decoder layers\n",
    "        self.decoder = Decoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers)\n",
    "        self.linear = nn.Linear(d_model, vocab_size)\n",
    "        self.dec_pos_encoding = PositionalEncoding(d_model, 1)  \n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    def forward(self, input_tokens, pad_mask=None, atn_mask=True):\n",
    "        # Move input tensors to the appropriate device\n",
    "        input_tokens = input_tokens.to(self.device)\n",
    "        pad_mask = pad_mask.to(self.device) if pad_mask is not None else None\n",
    "        max_sequence_length = input_tokens.shape[1]  # Fix this line to get the correct sequence length\n",
    "\n",
    "        # Get token embeddings from GPT-2 model and detach gradients\n",
    "        with torch.no_grad():\n",
    "            token_embeddings = self.gpt2(input_tokens)[0]\n",
    "        \n",
    "        # Add positional encodings\n",
    "        self.dec_pos_encoding = PositionalEncoding(d_model, max_sequence_length)\n",
    "        token_pos_encodings = self.dec_pos_encoding(device=self.device)\n",
    "        token_embeddings_with_pos = token_embeddings + token_pos_encodings.unsqueeze(0)\n",
    "\n",
    "        # Perform the rest of the forward pass\n",
    "        out = self.decoder(token_embeddings_with_pos, pad_mask, atn_mask)\n",
    "        out = self.linear(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a6cf72e-27dd-43ad-a175-1ad4a776774e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import BertTokenizer, BertModel\n",
    "# bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "\n",
    "# Load pre-trained GPT-2 tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# # Load pre-trained GPT-2 model\n",
    "# gpt2_model = GPT2Model.from_pretrained('gpt2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29b0d8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 768\n",
    "num_heads = 8\n",
    "drop_prob = 0.1\n",
    "batch_size = 128\n",
    "ffn_hidden = 768\n",
    "num_layers = 4\n",
    "vocab_size = vocab_size\n",
    "num_epochs = 100\n",
    "temperature = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d41ffa72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to get the device\n",
    "def get_device():\n",
    "    return torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "257d13af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (gpt2): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (layers): SequentialDecoder(\n",
       "      (0): DecoderLayer(\n",
       "        (self_attention1): MultiHeadAttention(\n",
       "          (qkv_layer): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (linear_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (self_attention2): MultiHeadAttention(\n",
       "          (qkv_layer): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (linear_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (ffn): PositionwiseFeedForward(\n",
       "          (linear1): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (linear2): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): DecoderLayer(\n",
       "        (self_attention1): MultiHeadAttention(\n",
       "          (qkv_layer): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (linear_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (self_attention2): MultiHeadAttention(\n",
       "          (qkv_layer): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (linear_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (ffn): PositionwiseFeedForward(\n",
       "          (linear1): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (linear2): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): DecoderLayer(\n",
       "        (self_attention1): MultiHeadAttention(\n",
       "          (qkv_layer): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (linear_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (self_attention2): MultiHeadAttention(\n",
       "          (qkv_layer): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (linear_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (ffn): PositionwiseFeedForward(\n",
       "          (linear1): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (linear2): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (3): DecoderLayer(\n",
       "        (self_attention1): MultiHeadAttention(\n",
       "          (qkv_layer): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (linear_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (self_attention2): MultiHeadAttention(\n",
       "          (qkv_layer): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (linear_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (ffn): PositionwiseFeedForward(\n",
       "          (linear1): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (linear2): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=768, out_features=50257, bias=True)\n",
       "  (dec_pos_encoding): PositionalEncoding()\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Move the model to the appropriate device\n",
    "transformer = Transformer(d_model, ffn_hidden, num_heads, drop_prob, num_layers, vocab_size)\n",
    "transformer.to(get_device())  # Move the model to GPU if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "120258ed-910d-4c94-b877-af96aff4c8b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 62,289,745 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(transformer):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "475ef085",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MyDataset(df, max_seq_len=128)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "50656624",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Batch [1/79], Loss: 10.951462745666504\n",
      "Epoch [1/100], Batch [51/79], Loss: 6.035002708435059\n",
      "Epoch [1/100], Total Loss: 6.85837719108485\n",
      "Epoch [2/100], Batch [1/79], Loss: 5.588253498077393\n",
      "Epoch [2/100], Batch [51/79], Loss: 5.212686538696289\n",
      "Epoch [2/100], Total Loss: 5.151911494098132\n",
      "Epoch [3/100], Batch [1/79], Loss: 4.780664920806885\n",
      "Epoch [3/100], Batch [51/79], Loss: 4.5213470458984375\n",
      "Epoch [3/100], Total Loss: 4.42597576032711\n",
      "Epoch [4/100], Batch [1/79], Loss: 4.200444221496582\n",
      "Epoch [4/100], Batch [51/79], Loss: 4.117523193359375\n",
      "Epoch [4/100], Total Loss: 3.9842699962326242\n",
      "Epoch [5/100], Batch [1/79], Loss: 3.8674051761627197\n",
      "Epoch [5/100], Batch [51/79], Loss: 3.858919382095337\n",
      "Epoch [5/100], Total Loss: 3.7057466808753676\n",
      "Epoch [6/100], Batch [1/79], Loss: 3.6400296688079834\n",
      "Epoch [6/100], Batch [51/79], Loss: 3.6512222290039062\n",
      "Epoch [6/100], Total Loss: 3.4937721173974534\n",
      "Epoch [7/100], Batch [1/79], Loss: 3.431352376937866\n",
      "Epoch [7/100], Batch [51/79], Loss: 3.465402603149414\n",
      "Epoch [7/100], Total Loss: 3.2963497397265855\n",
      "Epoch [8/100], Batch [1/79], Loss: 3.26652455329895\n",
      "Epoch [8/100], Batch [51/79], Loss: 3.2921478748321533\n",
      "Epoch [8/100], Total Loss: 3.1194493498983262\n",
      "Epoch [9/100], Batch [1/79], Loss: 3.085700511932373\n",
      "Epoch [9/100], Batch [51/79], Loss: 3.1143877506256104\n",
      "Epoch [9/100], Total Loss: 2.941196257555032\n",
      "Epoch [10/100], Batch [1/79], Loss: 2.90938138961792\n",
      "Epoch [10/100], Batch [51/79], Loss: 2.9320850372314453\n",
      "Epoch [10/100], Total Loss: 2.764171820652636\n",
      "Epoch [11/100], Batch [1/79], Loss: 2.7371294498443604\n",
      "Epoch [11/100], Batch [51/79], Loss: 2.7498624324798584\n",
      "Epoch [11/100], Total Loss: 2.587155465838275\n",
      "Epoch [12/100], Batch [1/79], Loss: 2.563957929611206\n",
      "Epoch [12/100], Batch [51/79], Loss: 2.5750906467437744\n",
      "Epoch [12/100], Total Loss: 2.4227130684671523\n",
      "Epoch [13/100], Batch [1/79], Loss: 2.3981781005859375\n",
      "Epoch [13/100], Batch [51/79], Loss: 2.4270195960998535\n",
      "Epoch [13/100], Total Loss: 2.2701727951629254\n",
      "Epoch [14/100], Batch [1/79], Loss: 2.2450077533721924\n",
      "Epoch [14/100], Batch [51/79], Loss: 2.2774405479431152\n",
      "Epoch [14/100], Total Loss: 2.128321452985836\n",
      "Epoch [15/100], Batch [1/79], Loss: 2.1110379695892334\n",
      "Epoch [15/100], Batch [51/79], Loss: 2.1350114345550537\n",
      "Epoch [15/100], Total Loss: 1.9966038329691826\n",
      "Epoch [16/100], Batch [1/79], Loss: 1.971881628036499\n",
      "Epoch [16/100], Batch [51/79], Loss: 2.0144431591033936\n",
      "Epoch [16/100], Total Loss: 1.8699029261552835\n",
      "Epoch [17/100], Batch [1/79], Loss: 1.8427166938781738\n",
      "Epoch [17/100], Batch [51/79], Loss: 1.8869690895080566\n",
      "Epoch [17/100], Total Loss: 1.7539694641209855\n",
      "Epoch [18/100], Batch [1/79], Loss: 1.7286075353622437\n",
      "Epoch [18/100], Batch [51/79], Loss: 1.7794686555862427\n",
      "Epoch [18/100], Total Loss: 1.6468165705475626\n",
      "Epoch [19/100], Batch [1/79], Loss: 1.634284257888794\n",
      "Epoch [19/100], Batch [51/79], Loss: 1.6920301914215088\n",
      "Epoch [19/100], Total Loss: 1.5532287902469877\n",
      "Epoch [20/100], Batch [1/79], Loss: 1.5482568740844727\n",
      "Epoch [20/100], Batch [51/79], Loss: 1.6030725240707397\n",
      "Epoch [20/100], Total Loss: 1.47119042843203\n",
      "Epoch [21/100], Batch [1/79], Loss: 1.4625318050384521\n",
      "Epoch [21/100], Batch [51/79], Loss: 1.5204246044158936\n",
      "Epoch [21/100], Total Loss: 1.3957259624819212\n",
      "Epoch [22/100], Batch [1/79], Loss: 1.3972088098526\n",
      "Epoch [22/100], Batch [51/79], Loss: 1.4518979787826538\n",
      "Epoch [22/100], Total Loss: 1.319778756250309\n",
      "Epoch [23/100], Batch [1/79], Loss: 1.3155097961425781\n",
      "Epoch [23/100], Batch [51/79], Loss: 1.3725781440734863\n",
      "Epoch [23/100], Total Loss: 1.2552255874947658\n",
      "Epoch [24/100], Batch [1/79], Loss: 1.2505210638046265\n",
      "Epoch [24/100], Batch [51/79], Loss: 1.3197013139724731\n",
      "Epoch [24/100], Total Loss: 1.201039904280554\n",
      "Epoch [25/100], Batch [1/79], Loss: 1.2023731470108032\n",
      "Epoch [25/100], Batch [51/79], Loss: 1.2528799772262573\n",
      "Epoch [25/100], Total Loss: 1.1390548189984093\n",
      "Epoch [26/100], Batch [1/79], Loss: 1.1327534914016724\n",
      "Epoch [26/100], Batch [51/79], Loss: 1.2036457061767578\n",
      "Epoch [26/100], Total Loss: 1.0853519620774668\n",
      "Epoch [27/100], Batch [1/79], Loss: 1.080437421798706\n",
      "Epoch [27/100], Batch [51/79], Loss: 1.1530325412750244\n",
      "Epoch [27/100], Total Loss: 1.0398957306825662\n",
      "Epoch [28/100], Batch [1/79], Loss: 1.0451669692993164\n",
      "Epoch [28/100], Batch [51/79], Loss: 1.1110135316848755\n",
      "Epoch [28/100], Total Loss: 0.9978599714327462\n",
      "Epoch [29/100], Batch [1/79], Loss: 0.9968566298484802\n",
      "Epoch [29/100], Batch [51/79], Loss: 1.0743138790130615\n",
      "Epoch [29/100], Total Loss: 0.9599700004239625\n",
      "Epoch [30/100], Batch [1/79], Loss: 0.9612838625907898\n",
      "Epoch [30/100], Batch [51/79], Loss: 1.0302033424377441\n",
      "Epoch [30/100], Total Loss: 0.927253142187867\n",
      "Epoch [31/100], Batch [1/79], Loss: 0.9305484294891357\n",
      "Epoch [31/100], Batch [51/79], Loss: 0.9884064793586731\n",
      "Epoch [31/100], Total Loss: 0.8940808901303932\n",
      "Epoch [32/100], Batch [1/79], Loss: 0.9006029963493347\n",
      "Epoch [32/100], Batch [51/79], Loss: 0.9539427161216736\n",
      "Epoch [32/100], Total Loss: 0.8651692463627344\n",
      "Epoch [33/100], Batch [1/79], Loss: 0.8638670444488525\n",
      "Epoch [33/100], Batch [51/79], Loss: 0.9310517907142639\n",
      "Epoch [33/100], Total Loss: 0.8388041325762302\n",
      "Epoch [34/100], Batch [1/79], Loss: 0.8414418697357178\n",
      "Epoch [34/100], Batch [51/79], Loss: 0.9068261981010437\n",
      "Epoch [34/100], Total Loss: 0.8131432355959204\n",
      "Epoch [35/100], Batch [1/79], Loss: 0.8225337266921997\n",
      "Epoch [35/100], Batch [51/79], Loss: 0.8732840418815613\n",
      "Epoch [35/100], Total Loss: 0.7888209499135802\n",
      "Epoch [36/100], Batch [1/79], Loss: 0.7976468205451965\n",
      "Epoch [36/100], Batch [51/79], Loss: 0.8587551712989807\n",
      "Epoch [36/100], Total Loss: 0.7682978861694094\n",
      "Epoch [37/100], Batch [1/79], Loss: 0.766667902469635\n",
      "Epoch [37/100], Batch [51/79], Loss: 0.8257846236228943\n",
      "Epoch [37/100], Total Loss: 0.7468306165707262\n",
      "Epoch [38/100], Batch [1/79], Loss: 0.7520856261253357\n",
      "Epoch [38/100], Batch [51/79], Loss: 0.8028327822685242\n",
      "Epoch [38/100], Total Loss: 0.7276919536952731\n",
      "Epoch [39/100], Batch [1/79], Loss: 0.7346695065498352\n",
      "Epoch [39/100], Batch [51/79], Loss: 0.7856389284133911\n",
      "Epoch [39/100], Total Loss: 0.7090339147591893\n",
      "Epoch [40/100], Batch [1/79], Loss: 0.7157190442085266\n",
      "Epoch [40/100], Batch [51/79], Loss: 0.7653535604476929\n",
      "Epoch [40/100], Total Loss: 0.6910645320445676\n",
      "Epoch [41/100], Batch [1/79], Loss: 0.6967838406562805\n",
      "Epoch [41/100], Batch [51/79], Loss: 0.7469789385795593\n",
      "Epoch [41/100], Total Loss: 0.6747832532170452\n",
      "Epoch [42/100], Batch [1/79], Loss: 0.6713565587997437\n",
      "Epoch [42/100], Batch [51/79], Loss: 0.7251003384590149\n",
      "Epoch [42/100], Total Loss: 0.6551366261666334\n",
      "Epoch [43/100], Batch [1/79], Loss: 0.6562666893005371\n",
      "Epoch [43/100], Batch [51/79], Loss: 0.7030243873596191\n",
      "Epoch [43/100], Total Loss: 0.6369505490683303\n",
      "Epoch [44/100], Batch [1/79], Loss: 0.6429651975631714\n",
      "Epoch [44/100], Batch [51/79], Loss: 0.6857656240463257\n",
      "Epoch [44/100], Total Loss: 0.620895846735073\n",
      "Epoch [45/100], Batch [1/79], Loss: 0.6268125772476196\n",
      "Epoch [45/100], Batch [51/79], Loss: 0.6695156693458557\n",
      "Epoch [45/100], Total Loss: 0.6061189674501177\n",
      "Epoch [46/100], Batch [1/79], Loss: 0.613126277923584\n",
      "Epoch [46/100], Batch [51/79], Loss: 0.6464356184005737\n",
      "Epoch [46/100], Total Loss: 0.5904584472315221\n",
      "Epoch [47/100], Batch [1/79], Loss: 0.5948163270950317\n",
      "Epoch [47/100], Batch [51/79], Loss: 0.635205090045929\n",
      "Epoch [47/100], Total Loss: 0.5787598326613631\n",
      "Epoch [48/100], Batch [1/79], Loss: 0.5821740031242371\n",
      "Epoch [48/100], Batch [51/79], Loss: 0.6282884478569031\n",
      "Epoch [48/100], Total Loss: 0.566225783168515\n",
      "Epoch [49/100], Batch [1/79], Loss: 0.5697664618492126\n",
      "Epoch [49/100], Batch [51/79], Loss: 0.6078895926475525\n",
      "Epoch [49/100], Total Loss: 0.5535024510154242\n",
      "Epoch [50/100], Batch [1/79], Loss: 0.5596429705619812\n",
      "Epoch [50/100], Batch [51/79], Loss: 0.5949673652648926\n",
      "Epoch [50/100], Total Loss: 0.5430301991821844\n",
      "Epoch [51/100], Batch [1/79], Loss: 0.545181930065155\n",
      "Epoch [51/100], Batch [51/79], Loss: 0.581684410572052\n",
      "Epoch [51/100], Total Loss: 0.5313866534565068\n",
      "Epoch [52/100], Batch [1/79], Loss: 0.5313904285430908\n",
      "Epoch [52/100], Batch [51/79], Loss: 0.5632583498954773\n",
      "Epoch [52/100], Total Loss: 0.5199987348876421\n",
      "Epoch [53/100], Batch [1/79], Loss: 0.5231449604034424\n",
      "Epoch [53/100], Batch [51/79], Loss: 0.5574420690536499\n",
      "Epoch [53/100], Total Loss: 0.5096089343858671\n",
      "Epoch [54/100], Batch [1/79], Loss: 0.5169371962547302\n",
      "Epoch [54/100], Batch [51/79], Loss: 0.5462278723716736\n",
      "Epoch [54/100], Total Loss: 0.4988097128800199\n",
      "Epoch [55/100], Batch [1/79], Loss: 0.5028008818626404\n",
      "Epoch [55/100], Batch [51/79], Loss: 0.5309966206550598\n",
      "Epoch [55/100], Total Loss: 0.4889097650405727\n",
      "Epoch [56/100], Batch [1/79], Loss: 0.4893292784690857\n",
      "Epoch [56/100], Batch [51/79], Loss: 0.5109084844589233\n",
      "Epoch [56/100], Total Loss: 0.47598437031235874\n",
      "Epoch [57/100], Batch [1/79], Loss: 0.4839858412742615\n",
      "Epoch [57/100], Batch [51/79], Loss: 0.502813994884491\n",
      "Epoch [57/100], Total Loss: 0.4648221337908431\n",
      "Epoch [58/100], Batch [1/79], Loss: 0.4686603248119354\n",
      "Epoch [58/100], Batch [51/79], Loss: 0.49630996584892273\n",
      "Epoch [58/100], Total Loss: 0.4485041180366202\n",
      "Epoch [59/100], Batch [1/79], Loss: 0.4502149224281311\n",
      "Epoch [59/100], Batch [51/79], Loss: 0.46574631333351135\n",
      "Epoch [59/100], Total Loss: 0.4275527495560767\n",
      "Epoch [60/100], Batch [1/79], Loss: 0.4199136197566986\n",
      "Epoch [60/100], Batch [51/79], Loss: 0.44054749608039856\n",
      "Epoch [60/100], Total Loss: 0.40652540854260893\n",
      "Epoch [61/100], Batch [1/79], Loss: 0.40558922290802\n",
      "Epoch [61/100], Batch [51/79], Loss: 0.4182632565498352\n",
      "Epoch [61/100], Total Loss: 0.3875898928204669\n",
      "Epoch [62/100], Batch [1/79], Loss: 0.3835291862487793\n",
      "Epoch [62/100], Batch [51/79], Loss: 0.40037041902542114\n",
      "Epoch [62/100], Total Loss: 0.3667164139355285\n",
      "Epoch [63/100], Batch [1/79], Loss: 0.365856409072876\n",
      "Epoch [63/100], Batch [51/79], Loss: 0.38110747933387756\n",
      "Epoch [63/100], Total Loss: 0.34992598497037647\n",
      "Epoch [64/100], Batch [1/79], Loss: 0.35385674238204956\n",
      "Epoch [64/100], Batch [51/79], Loss: 0.3664151728153229\n",
      "Epoch [64/100], Total Loss: 0.3358845578718789\n",
      "Epoch [65/100], Batch [1/79], Loss: 0.33999374508857727\n",
      "Epoch [65/100], Batch [51/79], Loss: 0.3533565402030945\n",
      "Epoch [65/100], Total Loss: 0.3239502042437656\n",
      "Epoch [66/100], Batch [1/79], Loss: 0.32941734790802\n",
      "Epoch [66/100], Batch [51/79], Loss: 0.33893588185310364\n",
      "Epoch [66/100], Total Loss: 0.31333884978784793\n",
      "Epoch [67/100], Batch [1/79], Loss: 0.3209741413593292\n",
      "Epoch [67/100], Batch [51/79], Loss: 0.33386334776878357\n",
      "Epoch [67/100], Total Loss: 0.30238797388310673\n",
      "Epoch [68/100], Batch [1/79], Loss: 0.30092182755470276\n",
      "Epoch [68/100], Batch [51/79], Loss: 0.31740665435791016\n",
      "Epoch [68/100], Total Loss: 0.29197772856377346\n",
      "Epoch [69/100], Batch [1/79], Loss: 0.29770368337631226\n",
      "Epoch [69/100], Batch [51/79], Loss: 0.3058016300201416\n",
      "Epoch [69/100], Total Loss: 0.28221890330314636\n",
      "Epoch [70/100], Batch [1/79], Loss: 0.28713029623031616\n",
      "Epoch [70/100], Batch [51/79], Loss: 0.29793581366539\n",
      "Epoch [70/100], Total Loss: 0.2740790397871899\n",
      "Epoch [71/100], Batch [1/79], Loss: 0.27285733819007874\n",
      "Epoch [71/100], Batch [51/79], Loss: 0.2874948978424072\n",
      "Epoch [71/100], Total Loss: 0.2654878313688538\n",
      "Epoch [72/100], Batch [1/79], Loss: 0.2628040015697479\n",
      "Epoch [72/100], Batch [51/79], Loss: 0.27894604206085205\n",
      "Epoch [72/100], Total Loss: 0.25751338521891004\n",
      "Epoch [73/100], Batch [1/79], Loss: 0.2601487934589386\n",
      "Epoch [73/100], Batch [51/79], Loss: 0.2767391502857208\n",
      "Epoch [73/100], Total Loss: 0.25135888668555245\n",
      "Epoch [74/100], Batch [1/79], Loss: 0.26261040568351746\n",
      "Epoch [74/100], Batch [51/79], Loss: 0.2605413496494293\n",
      "Epoch [74/100], Total Loss: 0.24471995390102833\n",
      "Epoch [75/100], Batch [1/79], Loss: 0.25139427185058594\n",
      "Epoch [75/100], Batch [51/79], Loss: 0.2522924542427063\n",
      "Epoch [75/100], Total Loss: 0.23831509883645213\n",
      "Epoch [76/100], Batch [1/79], Loss: 0.24802209436893463\n",
      "Epoch [76/100], Batch [51/79], Loss: 0.24872693419456482\n",
      "Epoch [76/100], Total Loss: 0.23220379820352868\n",
      "Epoch [77/100], Batch [1/79], Loss: 0.2357645332813263\n",
      "Epoch [77/100], Batch [51/79], Loss: 0.23984941840171814\n",
      "Epoch [77/100], Total Loss: 0.22668536101715475\n",
      "Epoch [78/100], Batch [1/79], Loss: 0.23689138889312744\n",
      "Epoch [78/100], Batch [51/79], Loss: 0.23442164063453674\n",
      "Epoch [78/100], Total Loss: 0.22017577389561677\n",
      "Epoch [79/100], Batch [1/79], Loss: 0.23470714688301086\n",
      "Epoch [79/100], Batch [51/79], Loss: 0.23060059547424316\n",
      "Epoch [79/100], Total Loss: 0.21412526042778282\n",
      "Epoch [80/100], Batch [1/79], Loss: 0.22510415315628052\n",
      "Epoch [80/100], Batch [51/79], Loss: 0.2248065173625946\n",
      "Epoch [80/100], Total Loss: 0.20753679563633248\n",
      "Epoch [81/100], Batch [1/79], Loss: 0.2116730511188507\n",
      "Epoch [81/100], Batch [51/79], Loss: 0.20792336761951447\n",
      "Epoch [81/100], Total Loss: 0.20053749255647388\n",
      "Epoch [82/100], Batch [1/79], Loss: 0.2038232833147049\n",
      "Epoch [82/100], Batch [51/79], Loss: 0.20536068081855774\n",
      "Epoch [82/100], Total Loss: 0.1923696014417123\n",
      "Epoch [83/100], Batch [1/79], Loss: 0.19838587939739227\n",
      "Epoch [83/100], Batch [51/79], Loss: 0.19182661175727844\n",
      "Epoch [83/100], Total Loss: 0.18242582883921604\n",
      "Epoch [84/100], Batch [1/79], Loss: 0.18307307362556458\n",
      "Epoch [84/100], Batch [51/79], Loss: 0.18822354078292847\n",
      "Epoch [84/100], Total Loss: 0.17302099084835262\n",
      "Epoch [85/100], Batch [1/79], Loss: 0.1772627830505371\n",
      "Epoch [85/100], Batch [51/79], Loss: 0.18013383448123932\n",
      "Epoch [85/100], Total Loss: 0.1631953670772948\n",
      "Epoch [86/100], Batch [1/79], Loss: 0.16875781118869781\n",
      "Epoch [86/100], Batch [51/79], Loss: 0.16728712618350983\n",
      "Epoch [86/100], Total Loss: 0.15563304840198047\n",
      "Epoch [87/100], Batch [1/79], Loss: 0.15771867334842682\n",
      "Epoch [87/100], Batch [51/79], Loss: 0.15876731276512146\n",
      "Epoch [87/100], Total Loss: 0.14899790567593485\n",
      "Epoch [88/100], Batch [1/79], Loss: 0.15206028521060944\n",
      "Epoch [88/100], Batch [51/79], Loss: 0.15113413333892822\n",
      "Epoch [88/100], Total Loss: 0.14256033746973623\n",
      "Epoch [89/100], Batch [1/79], Loss: 0.1473638415336609\n",
      "Epoch [89/100], Batch [51/79], Loss: 0.14444980025291443\n",
      "Epoch [89/100], Total Loss: 0.1349701334639818\n",
      "Epoch [90/100], Batch [1/79], Loss: 0.13516315817832947\n",
      "Epoch [90/100], Batch [51/79], Loss: 0.13467922806739807\n",
      "Epoch [90/100], Total Loss: 0.12903130511882938\n",
      "Epoch [91/100], Batch [1/79], Loss: 0.1337755173444748\n",
      "Epoch [91/100], Batch [51/79], Loss: 0.13134489953517914\n",
      "Epoch [91/100], Total Loss: 0.12389862006883832\n",
      "Epoch [92/100], Batch [1/79], Loss: 0.1273663491010666\n",
      "Epoch [92/100], Batch [51/79], Loss: 0.12424831092357635\n",
      "Epoch [92/100], Total Loss: 0.1184166707334262\n",
      "Epoch [93/100], Batch [1/79], Loss: 0.12031262367963791\n",
      "Epoch [93/100], Batch [51/79], Loss: 0.11494889855384827\n",
      "Epoch [93/100], Total Loss: 0.11213055264817763\n",
      "Epoch [94/100], Batch [1/79], Loss: 0.11851876974105835\n",
      "Epoch [94/100], Batch [51/79], Loss: 0.11580415070056915\n",
      "Epoch [94/100], Total Loss: 0.10845490877481201\n",
      "Epoch [95/100], Batch [1/79], Loss: 0.11670779436826706\n",
      "Epoch [95/100], Batch [51/79], Loss: 0.10852374136447906\n",
      "Epoch [95/100], Total Loss: 0.10309847973617195\n",
      "Epoch [96/100], Batch [1/79], Loss: 0.10390033572912216\n",
      "Epoch [96/100], Batch [51/79], Loss: 0.10417414456605911\n",
      "Epoch [96/100], Total Loss: 0.09977609713715088\n",
      "Epoch [97/100], Batch [1/79], Loss: 0.0993274450302124\n",
      "Epoch [97/100], Batch [51/79], Loss: 0.09937640279531479\n",
      "Epoch [97/100], Total Loss: 0.09506147805176958\n",
      "Epoch [98/100], Batch [1/79], Loss: 0.09893737733364105\n",
      "Epoch [98/100], Batch [51/79], Loss: 0.10178720951080322\n",
      "Epoch [98/100], Total Loss: 0.09185281933485707\n",
      "Epoch [99/100], Batch [1/79], Loss: 0.09580933302640915\n",
      "Epoch [99/100], Batch [51/79], Loss: 0.09260112792253494\n",
      "Epoch [99/100], Total Loss: 0.08796441646976562\n",
      "Epoch [100/100], Batch [1/79], Loss: 0.09341339021921158\n",
      "Epoch [100/100], Batch [51/79], Loss: 0.0873691737651825\n",
      "Epoch [100/100], Total Loss: 0.08520562841875266\n"
     ]
    }
   ],
   "source": [
    "# Define the optimizer and scheduler\n",
    "optim = torch.optim.Adam(transformer.parameters(), lr=1e-4)\n",
    "#scheduler = StepLR(optim, step_size=15, gamma=0.1)  # Learning rate decreases by a factor of 0.1 every 5 epochs\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Iterate over epochs\n",
    "for epoch in range(num_epochs):\n",
    "    transformer.train()  # Set the model to training mode\n",
    "    total_loss = 0\n",
    "    for batch_num, batch in enumerate(dataloader):\n",
    "        input_tokens, target_tokens, pad_mask = batch['input_tokens'], batch['target_tokens'], batch['mask']\n",
    "        \n",
    "        # Move tensors to GPU if available\n",
    "        input_tokens = input_tokens.to(get_device())\n",
    "        target_tokens = target_tokens.to(get_device())\n",
    "        pad_mask = pad_mask.to(get_device())\n",
    "        \n",
    "        optim.zero_grad()\n",
    "        predictions = transformer(input_tokens, pad_mask)\n",
    "        loss = criterion(predictions.view(-1, vocab_size), target_tokens.view(-1))\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Print loss after each batch\n",
    "        print_every = 50  # Define how often to print the loss\n",
    "        if batch_num % print_every == 0:\n",
    "           print(f'Epoch [{epoch + 1}/{num_epochs}], Batch [{batch_num + 1}/{len(dataloader)}], Loss: {loss.item()}')\n",
    "    \n",
    "    # Print total loss after each epoch\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Total Loss: {total_loss/len(dataloader)}')\n",
    "    \n",
    "    # Update the learning rate\n",
    "    #scheduler.step()\n",
    "    \n",
    "    # Save the model after each epoch\n",
    "    model_path = '/workspace/transformer_v2.pth'\n",
    "    torch.save(transformer.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "289778ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated sequence: once upon a time, Anna got to try a smelly blouse and a swimsuit! She started to dance and dance and dance and dance!\"\n",
      "\n",
      "Anna didn't know what to do. She ran to her mom and said, \"I know I don't want to dance.\"\n",
      "\n",
      "Mom finally came running and helped her dance. She didn't dance! She never did!\n",
      "\n",
      "Once again, Anna felt ashamed. She ran to her mom and saw a big rainbow. She couldn't stop. She ran\n"
     ]
    }
   ],
   "source": [
    "def inference(transformer, tokenizer, starting_word, max_length, temperature=1.0):\n",
    "    \n",
    "    transformer.eval()\n",
    "    # Convert starting and ending words to token IDs\n",
    "    starting_token_ids = tokenizer.encode(starting_word)\n",
    "\n",
    "    # Convert token IDs to tensor\n",
    "    input_tensor = torch.tensor(starting_token_ids).unsqueeze(0).to(get_device())\n",
    "\n",
    "    # Generate tokens until ending word is reached or maximum length is reached\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            # Forward pass through the model\n",
    "            output = transformer(input_tensor)\n",
    "\n",
    "            # Apply temperature scaling to the logits\n",
    "            scaled_output = output / temperature\n",
    "\n",
    "            # Get the last predicted token\n",
    "            last_token = scaled_output.argmax(dim=-1)[:, -1]\n",
    "\n",
    "            # Append the last token to the input tensor\n",
    "            last_token = last_token.unsqueeze(0).to(input_tensor.device)  # Ensure last_token is on the same device as input_tensor\n",
    "            input_tensor = torch.cat([input_tensor, last_token], dim=-1)\n",
    "\n",
    "            # Check if the ending word is reached\n",
    "            if (last_token == tokenizer.eos_token_id):\n",
    "                break\n",
    "\n",
    "    # Decode the generated tokens\n",
    "    generated_text = tokenizer.decode(input_tensor.squeeze().tolist())\n",
    "\n",
    "    return generated_text\n",
    "\n",
    "# Define the path where the model is saved\n",
    "model_path = '/workspace/transformer_v2.pth'\n",
    "\n",
    "# Load the model from the saved file\n",
    "transformer.load_state_dict(torch.load(model_path))\n",
    "\n",
    "# Now you can use the inference function with the loaded and evaluated model\n",
    "starting_word = \"once upon a time,\"\n",
    "max_length = 100\n",
    "temperature = 0.8\n",
    "\n",
    "generated_sequence = inference(transformer, tokenizer, starting_word\n",
    "                               , max_length, temperature)\n",
    "print(\"Generated sequence:\", generated_sequence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2a1867-3820-44ce-a617-be47ee57bad9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
